{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ac7c6721",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import one_hot,Tokenizer\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b87c16b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "a = torch.randn((64, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3e822152",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.1044)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "be3d80b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.1381)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(a * 1/64**0.5).std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "65fbfe06",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.125"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1/64**0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0a9e71c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "48790d02",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((100,),\n",
       " array([0.001     , 0.01109091, 0.02118182, 0.03127273, 0.04136364,\n",
       "        0.05145455, 0.06154545, 0.07163636, 0.08172727, 0.09181818,\n",
       "        0.10190909, 0.112     , 0.12209091, 0.13218182, 0.14227273,\n",
       "        0.15236364, 0.16245455, 0.17254545, 0.18263636, 0.19272727,\n",
       "        0.20281818, 0.21290909, 0.223     , 0.23309091, 0.24318182,\n",
       "        0.25327273, 0.26336364, 0.27345455, 0.28354545, 0.29363636,\n",
       "        0.30372727, 0.31381818, 0.32390909, 0.334     , 0.34409091,\n",
       "        0.35418182, 0.36427273, 0.37436364, 0.38445455, 0.39454545,\n",
       "        0.40463636, 0.41472727, 0.42481818, 0.43490909, 0.445     ,\n",
       "        0.45509091, 0.46518182, 0.47527273, 0.48536364, 0.49545455,\n",
       "        0.50554545, 0.51563636, 0.52572727, 0.53581818, 0.54590909,\n",
       "        0.556     , 0.56609091, 0.57618182, 0.58627273, 0.59636364,\n",
       "        0.60645455, 0.61654545, 0.62663636, 0.63672727, 0.64681818,\n",
       "        0.65690909, 0.667     , 0.67709091, 0.68718182, 0.69727273,\n",
       "        0.70736364, 0.71745455, 0.72754545, 0.73763636, 0.74772727,\n",
       "        0.75781818, 0.76790909, 0.778     , 0.78809091, 0.79818182,\n",
       "        0.80827273, 0.81836364, 0.82845455, 0.83854545, 0.84863636,\n",
       "        0.85872727, 0.86881818, 0.87890909, 0.889     , 0.89909091,\n",
       "        0.90918182, 0.91927273, 0.92936364, 0.93945455, 0.94954545,\n",
       "        0.95963636, 0.96972727, 0.97981818, 0.98990909, 1.        ]))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = np.linspace(0.001, 1, 100)\n",
    "b = np.linspace(-3, 0, 100)\n",
    "b = 10**b\n",
    "a.shape, a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1a7af97a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((100,),\n",
       " array([0.001     , 0.00107227, 0.00114976, 0.00123285, 0.00132194,\n",
       "        0.00141747, 0.00151991, 0.00162975, 0.00174753, 0.00187382,\n",
       "        0.00200923, 0.00215443, 0.00231013, 0.00247708, 0.00265609,\n",
       "        0.00284804, 0.00305386, 0.00327455, 0.00351119, 0.00376494,\n",
       "        0.00403702, 0.00432876, 0.00464159, 0.00497702, 0.0053367 ,\n",
       "        0.00572237, 0.00613591, 0.00657933, 0.0070548 , 0.00756463,\n",
       "        0.00811131, 0.00869749, 0.00932603, 0.01      , 0.01072267,\n",
       "        0.01149757, 0.01232847, 0.01321941, 0.01417474, 0.01519911,\n",
       "        0.01629751, 0.01747528, 0.01873817, 0.02009233, 0.02154435,\n",
       "        0.0231013 , 0.02477076, 0.02656088, 0.02848036, 0.03053856,\n",
       "        0.03274549, 0.03511192, 0.03764936, 0.04037017, 0.04328761,\n",
       "        0.04641589, 0.04977024, 0.05336699, 0.05722368, 0.06135907,\n",
       "        0.06579332, 0.07054802, 0.07564633, 0.08111308, 0.0869749 ,\n",
       "        0.09326033, 0.1       , 0.10722672, 0.1149757 , 0.12328467,\n",
       "        0.13219411, 0.14174742, 0.15199111, 0.16297508, 0.17475284,\n",
       "        0.18738174, 0.2009233 , 0.21544347, 0.23101297, 0.24770764,\n",
       "        0.26560878, 0.28480359, 0.30538555, 0.32745492, 0.35111917,\n",
       "        0.37649358, 0.40370173, 0.43287613, 0.46415888, 0.49770236,\n",
       "        0.53366992, 0.57223677, 0.61359073, 0.65793322, 0.70548023,\n",
       "        0.75646333, 0.81113083, 0.869749  , 0.93260335, 1.        ]))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b.shape, b"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea33e297",
   "metadata": {},
   "source": [
    "### <font color='Brown'>We are training LLM based on character-based level (instead of a token(word) like ChatGPT does.)</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e63cacb",
   "metadata": {},
   "source": [
    "# Loading Data and Analysing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6dbace8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('input.txt', 'r') as f:\n",
    "    text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "873967d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of text in characters 1115394\n"
     ]
    }
   ],
   "source": [
    "print('Length of text in characters', len(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "37d59c24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "All:\n",
      "Speak, speak.\n",
      "\n",
      "First Citizen:\n",
      "You are all resolved rather to die than to famish?\n",
      "\n",
      "All:\n",
      "Resolved. resolved.\n",
      "\n",
      "First Citizen:\n",
      "First, you\n"
     ]
    }
   ],
   "source": [
    "print(text[:200])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de95c9b9",
   "metadata": {},
   "source": [
    "# Vocabulary; All the unique characters in the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f4dcad3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total uniques characters: 65\n",
      "All unique characters in the text: \n",
      " !$&',-.3:;?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\n"
     ]
    }
   ],
   "source": [
    "vocab = sorted(list(set(text)))\n",
    "vocab_size = len(vocab)\n",
    "print('Total uniques characters:', vocab_size)\n",
    "print('All unique characters in the text:', ''.join(vocab))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b88b760c",
   "metadata": {},
   "source": [
    "# Character to Integer encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "911a0bad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab Size: 65\n",
      "[46, 47, 1, 58, 46, 43, 56, 43]\n",
      "hi there\n"
     ]
    }
   ],
   "source": [
    "# create a mapping for each character to integer\n",
    "stoi = {ch:i for i, ch in enumerate(vocab)}\n",
    "itos = {i:ch for i, ch in enumerate(vocab)}\n",
    "encode = lambda s: [stoi[c] for c in s] # encoder: take a string, output a list on integers\n",
    "decode = lambda l: ''.join([itos[i] for i in l]) # encoder: take a list on integers, output a string\n",
    "\n",
    "print(\"Vocab Size:\", vocab_size) \n",
    "print(encode('hi there'))\n",
    "print(decode(encode('hi there')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "63289c35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab Size: 50257\n",
      "[71, 4178, 612]\n",
      "hii there\n"
     ]
    }
   ],
   "source": [
    "# EXTRA EXPLANATION\n",
    "'''\n",
    "encod_corp = one_hot(text, vocab_size)\n",
    "we could've done this if we were to make the model based on tokens instead of single char\n",
    "\n",
    "In practice most of the LLMs like ChatGPT uses sub-word encoding.\n",
    "Meaning instead of encoding \"hii there\" to char level like this: [1,2,2,3,4,5,6,7,6]\n",
    "or instead of encoding \"hii there\" to token level like this: [1,2]\n",
    "sub-word encodes \"hii there\" to sub-token level like this: [1,2,3] --> 1:hi, 2:i, 3:the\n",
    "\n",
    "This is a trade-off between code book size (vocabulary) and sequences of output intergers\n",
    "'''\n",
    "# OpenAI tiktoken for sub-word encoding\n",
    "from tiktoken import get_encoding\n",
    "enc = get_encoding('gpt2')\n",
    "print(\"Vocab Size:\", enc.n_vocab) # We can see vocab size if based on char level will just be around 65, but on sub-word level it will have a ton of a size\n",
    "print(enc.encode('hii there')) # output sequences is just length of 3\n",
    "print(enc.decode(enc.encode('hii there')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9dde416d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1115394]) torch.int64\n",
      "tensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 14, 43, 44,\n",
      "        53, 56, 43,  1, 61, 43,  1, 54, 56, 53, 41, 43, 43, 42,  1, 39, 52, 63,\n",
      "         1, 44, 59, 56, 58, 46, 43, 56,  6,  1, 46, 43, 39, 56,  1, 51, 43,  1,\n",
      "        57, 54, 43, 39, 49,  8,  0,  0, 13, 50, 50, 10,  0, 31, 54, 43, 39, 49,\n",
      "         6,  1, 57, 54, 43, 39, 49,  8,  0,  0, 18, 47, 56, 57, 58,  1, 15, 47,\n",
      "        58, 47, 64, 43, 52, 10,  0, 37, 53, 59,  1, 39, 56, 43,  1, 39, 50, 50,\n",
      "         1, 56, 43, 57, 53, 50, 60, 43, 42,  1, 56, 39, 58, 46, 43, 56,  1, 58,\n",
      "        53,  1, 42, 47, 43,  1, 58, 46, 39, 52,  1, 58, 53,  1, 44, 39, 51, 47,\n",
      "        57, 46, 12,  0,  0, 13, 50, 50, 10,  0, 30, 43, 57, 53, 50, 60, 43, 42,\n",
      "         8,  1, 56, 43, 57, 53, 50, 60, 43, 42,  8,  0,  0, 18, 47, 56, 57, 58,\n",
      "         1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 18, 47, 56, 57, 58,  6,  1, 63,\n",
      "        53, 59])\n"
     ]
    }
   ],
   "source": [
    "# Let's encode the entire text dataset and store in into torch tensor\n",
    "import torch\n",
    "data = torch.tensor(encode(text), dtype=torch.long) # torch.long: 64 bit integer tensor\n",
    "print(data.shape, data.dtype)\n",
    "print(data[:200])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9476559d",
   "metadata": {},
   "source": [
    "# Split into Train and Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f74fb9c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "n = int(0.9*len(data))\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4b8c438",
   "metadata": {},
   "source": [
    "# Context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cdc125e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "block_size = 9 # context_length\n",
    "'''\n",
    "if as long as we have 1 char, the model know what would come next\n",
    "here we'll have 9 Examples.\n",
    "if model sees 18 it'll has have some idea what will come next i.e., 47\n",
    "if model sees 18 and 47, it would likely output 56\n",
    "and so on..\n",
    "\n",
    "Remember this is just 1 block, will gonna feed whole data in blocks of 9,\n",
    "so likely 18 can be followed by any other character.\n",
    "If model will have more context like 18, 47, 56, 57, etc. then it might have high chance to predict 58 to next char\n",
    "\n",
    "Here let consider only this much dataset we trained the model on, \n",
    "now if the model sees 58 in real application has 2 options for next outcome 1 or 47\n",
    "It will only be decided clearly if it sees more context like: ..., 57, 58 OR ..., 47, 58\n",
    "if it sees the former one, then it will output: 1\n",
    "or if it sees the latter one, then it will output: 47\n",
    "'''\n",
    "train_data[:block_size+1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c5db1da8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. When the input is tensor([18]), the Target is 47\n",
      "2. When the input is tensor([18, 47]), the Target is 56\n",
      "3. When the input is tensor([18, 47, 56]), the Target is 57\n",
      "4. When the input is tensor([18, 47, 56, 57]), the Target is 58\n",
      "5. When the input is tensor([18, 47, 56, 57, 58]), the Target is 1\n",
      "6. When the input is tensor([18, 47, 56, 57, 58,  1]), the Target is 15\n",
      "7. When the input is tensor([18, 47, 56, 57, 58,  1, 15]), the Target is 47\n",
      "8. When the input is tensor([18, 47, 56, 57, 58,  1, 15, 47]), the Target is 58\n",
      "9. When the input is tensor([18, 47, 56, 57, 58,  1, 15, 47, 58]), the Target is 47\n"
     ]
    }
   ],
   "source": [
    "# EXTRAS\n",
    "x = train_data[:block_size+1]\n",
    "y = train_data[1:block_size+1]\n",
    "for i, t in enumerate(range(block_size)):\n",
    "    context = x[:t+1]\n",
    "    target = y[t]\n",
    "    print(f'{i+1}. When the input is {context}, the Target is {target}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d847db6",
   "metadata": {},
   "source": [
    "# Train on Batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "48ad20c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input: torch.Size([4, 8]) \n",
      " tensor([[24, 43, 58,  5, 57,  1, 46, 43],\n",
      "        [44, 53, 56,  1, 58, 46, 39, 58],\n",
      "        [52, 58,  1, 58, 46, 39, 58,  1],\n",
      "        [25, 17, 27, 10,  0, 21,  1, 54]])\n",
      "target: torch.Size([4, 8]) \n",
      " tensor([[43, 58,  5, 57,  1, 46, 43, 39],\n",
      "        [53, 56,  1, 58, 46, 39, 58,  1],\n",
      "        [58,  1, 58, 46, 39, 58,  1, 46],\n",
      "        [17, 27, 10,  0, 21,  1, 54, 39]])\n",
      "\n",
      "----\n",
      "\n",
      "When the input is [24], the Target is 43\n",
      "When the input is [24, 43], the Target is 58\n",
      "When the input is [24, 43, 58], the Target is 5\n",
      "When the input is [24, 43, 58, 5], the Target is 57\n",
      "When the input is [24, 43, 58, 5, 57], the Target is 1\n",
      "When the input is [24, 43, 58, 5, 57, 1], the Target is 46\n",
      "When the input is [24, 43, 58, 5, 57, 1, 46], the Target is 43\n",
      "When the input is [24, 43, 58, 5, 57, 1, 46, 43], the Target is 39\n",
      "When the input is [44], the Target is 53\n",
      "When the input is [44, 53], the Target is 56\n",
      "When the input is [44, 53, 56], the Target is 1\n",
      "When the input is [44, 53, 56, 1], the Target is 58\n",
      "When the input is [44, 53, 56, 1, 58], the Target is 46\n",
      "When the input is [44, 53, 56, 1, 58, 46], the Target is 39\n",
      "When the input is [44, 53, 56, 1, 58, 46, 39], the Target is 58\n",
      "When the input is [44, 53, 56, 1, 58, 46, 39, 58], the Target is 1\n",
      "When the input is [52], the Target is 58\n",
      "When the input is [52, 58], the Target is 1\n",
      "When the input is [52, 58, 1], the Target is 58\n",
      "When the input is [52, 58, 1, 58], the Target is 46\n",
      "When the input is [52, 58, 1, 58, 46], the Target is 39\n",
      "When the input is [52, 58, 1, 58, 46, 39], the Target is 58\n",
      "When the input is [52, 58, 1, 58, 46, 39, 58], the Target is 1\n",
      "When the input is [52, 58, 1, 58, 46, 39, 58, 1], the Target is 46\n",
      "When the input is [25], the Target is 17\n",
      "When the input is [25, 17], the Target is 27\n",
      "When the input is [25, 17, 27], the Target is 10\n",
      "When the input is [25, 17, 27, 10], the Target is 0\n",
      "When the input is [25, 17, 27, 10, 0], the Target is 21\n",
      "When the input is [25, 17, 27, 10, 0, 21], the Target is 1\n",
      "When the input is [25, 17, 27, 10, 0, 21, 1], the Target is 54\n",
      "When the input is [25, 17, 27, 10, 0, 21, 1, 54], the Target is 39\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(1337)\n",
    "batch_size = 4 # how many independent sequences will we process in parallel?\n",
    "block_size = 8 # what is the maximum context length for predictions?\n",
    "\n",
    "def get_batch(batch_size, block_size, split='train'):\n",
    "    # Generate a small batch of data of input x and target y\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
    "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
    "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
    "    return x, y\n",
    "\n",
    "xb, yb = get_batch(batch_size, block_size)\n",
    "print('input:', xb.shape, '\\n', xb)\n",
    "print('target:', yb.shape, '\\n', yb)\n",
    "print('\\n----\\n')\n",
    "\n",
    "for b in range(batch_size): # batch dimension\n",
    "    for t in range(block_size): # time dimension\n",
    "        context = xb[b, :t+1]\n",
    "        target = yb[b, t]\n",
    "        print(f'When the input is {context.tolist()}, the Target is {target}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "828fe71f",
   "metadata": {},
   "source": [
    "# Make Model: Understanding forward() method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2e23ac3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape: torch.Size([32, 65])\n",
      "tensor([[-1.5101, -0.0948,  1.0927,  0.1505,  1.6347, -0.0518,  0.4996,  0.7216,\n",
      "         -0.8968, -0.4122,  1.0030,  0.8508,  0.2178,  0.0328, -0.1699,  1.0659,\n",
      "         -0.6177,  1.1824,  0.0214, -0.2154, -1.4623,  2.1707,  0.1624,  1.0296,\n",
      "          0.4154,  0.6207,  0.2341, -0.0326,  1.0124,  1.5122, -0.3359,  0.2456,\n",
      "          1.8682,  0.7536, -0.1177, -0.1967, -0.9552, -0.8995, -0.9583, -0.5945,\n",
      "          0.1321, -0.5406,  0.1405, -0.7321,  1.1796,  1.3316, -0.2094,  0.0960,\n",
      "          0.9040, -0.4032,  0.3027, -0.8034, -1.2537, -1.5195,  0.7446,  1.1914,\n",
      "         -0.8061, -0.6290,  1.2447, -2.4400,  0.8408, -0.3993, -0.6126, -0.6597,\n",
      "          0.7624],\n",
      "        [ 0.3323, -0.0872, -0.7470, -0.6074,  0.3418,  0.5343,  0.3957, -0.4919,\n",
      "         -0.0894, -1.3886,  1.2835, -0.3975,  2.0152,  1.6773, -0.3833,  1.5728,\n",
      "          1.9458,  0.7247, -0.4834, -0.3263,  0.3193, -0.4198, -0.6435, -0.3311,\n",
      "          0.7554, -1.2385,  0.4067,  0.9982, -0.6511,  1.2450,  0.2804,  0.8371,\n",
      "         -0.4119,  0.2115, -0.6240,  0.0203, -0.3418,  1.4934,  1.7307,  1.3354,\n",
      "         -0.2712,  0.4902,  0.6600, -1.6321, -0.7858,  1.7688,  2.6160, -0.5767,\n",
      "         -0.3628, -2.7428,  0.7428,  0.0737,  0.2050, -0.5497,  2.1261, -0.9240,\n",
      "          0.1048,  0.8324,  1.4287, -0.7789,  2.9275, -0.8525, -0.6716, -0.9572,\n",
      "         -0.9594]], grad_fn=<SliceBackward0>)\n",
      "Loss: tensor(4.8786, grad_fn=<NllLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "torch.manual_seed(1337)\n",
    "\n",
    "# Check out my Notebook on Embedding Layers: https://www.kaggle.com/code/lunaticsain/docs-embedding-layer-keras\n",
    "\n",
    "class BigramLanguageModel(nn.Module):\n",
    "    def __init__ (self, vocab_size):\n",
    "        super().__init__()\n",
    "        # each token directly reads of the logits for the next token from a lookup table\n",
    "        '''The size of the embedding vector (embedding_dim) for each word or token, in this case is set to vocab_size, \n",
    "            meaning each word or token will be represented by an vocab_size-dimensional embedding vector.\n",
    "        '''\n",
    "        self.token_embedding_table = nn.Embedding(num_embeddings=vocab_size, embedding_dim=vocab_size) # this will make a table of 65x65\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        # idx and targets are both (B, T) of tensor of interger\n",
    "        # each element in idx say 24, will be embedded in the sequence as a 65-dimensional vector\n",
    "        logits = self.token_embedding_table(idx) # (B, T, C) => Batch(batch_size), Time(block_size), Channel(vocab_size)\n",
    "        \n",
    "        # PyTorch cross_entropy expect to be in the shape of B, C\n",
    "        B, T, C = logits.shape\n",
    "        logits = logits.view(B*T, C) # reshaping it as 2-dimension\n",
    "        targets = targets.view(-1) # reshaping it as single dimension\n",
    "        loss = nn.functional.cross_entropy(logits, targets)\n",
    "        \n",
    "        return logits, loss\n",
    "\n",
    "m = BigramLanguageModel(vocab_size)\n",
    "logits, loss = m(xb, yb) # forward()\n",
    "print(\"Shape:\", logits.shape)\n",
    "print(logits[:2])\n",
    "print(\"Loss:\", loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9afafa32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x: tensor([-1.5101, -0.0948,  1.0927,  0.1505,  1.6347, -0.0518,  0.4996,  0.7216,\n",
      "        -0.8968, -0.4122,  1.0030,  0.8508,  0.2178,  0.0328, -0.1699,  1.0659,\n",
      "        -0.6177,  1.1824,  0.0214, -0.2154, -1.4623,  2.1707,  0.1624,  1.0296,\n",
      "         0.4154,  0.6207,  0.2341, -0.0326,  1.0124,  1.5122, -0.3359,  0.2456,\n",
      "         1.8682,  0.7536, -0.1177, -0.1967, -0.9552, -0.8995, -0.9583, -0.5945,\n",
      "         0.1321, -0.5406,  0.1405, -0.7321,  1.1796,  1.3316, -0.2094,  0.0960,\n",
      "         0.9040, -0.4032,  0.3027, -0.8034, -1.2537, -1.5195,  0.7446,  1.1914,\n",
      "        -0.8061, -0.6290,  1.2447, -2.4400,  0.8408, -0.3993, -0.6126, -0.6597,\n",
      "         0.7624], grad_fn=<SelectBackward0>)\n",
      "y: tensor(43)\n",
      "--------\n",
      "\n",
      "BUILT-IN:\n",
      "\tCross-Entropy: tensor(5.4015, grad_fn=<NllLossBackward0>) \n",
      "\n",
      "MANUAL:\n",
      "\tprobabilities: tensor([0.0021, 0.0085, 0.0280, 0.0109, 0.0481, 0.0089, 0.0155, 0.0193, 0.0038,\n",
      "        0.0062, 0.0256, 0.0220, 0.0117, 0.0097, 0.0079, 0.0272, 0.0051, 0.0306,\n",
      "        0.0096, 0.0076, 0.0022, 0.0822, 0.0110, 0.0263, 0.0142, 0.0174, 0.0119,\n",
      "        0.0091, 0.0258, 0.0425, 0.0067, 0.0120, 0.0607, 0.0199, 0.0083, 0.0077,\n",
      "        0.0036, 0.0038, 0.0036, 0.0052, 0.0107, 0.0055, 0.0108, 0.0045, 0.0305,\n",
      "        0.0355, 0.0076, 0.0103, 0.0232, 0.0063, 0.0127, 0.0042, 0.0027, 0.0021,\n",
      "        0.0197, 0.0309, 0.0042, 0.0050, 0.0326, 0.0008, 0.0217, 0.0063, 0.0051,\n",
      "        0.0048, 0.0201], grad_fn=<SoftmaxBackward0>)\n",
      "\tpredicted_prob: tensor(0.0045, grad_fn=<SelectBackward0>)\n",
      "\tCross-Entropy: 5.4014746718075735\n"
     ]
    }
   ],
   "source": [
    "# EXTRAS: Calculation of cross-entropy (it is same as negative-Log-Likelihood (Avg))\n",
    "x = logits[0] #embeddings of 1st alphabet/Time step i.e, emb of xb[0,0]\n",
    "y = yb[0,0]\n",
    "print('x:', x)\n",
    "print('y:', y)\n",
    "print('--------\\n')\n",
    "\n",
    "print('BUILT-IN:')\n",
    "print('\\tCross-Entropy:', nn.functional.cross_entropy(x, y), '\\n')\n",
    "\n",
    "print('MANUAL:')\n",
    "## Step 1: softmax probability\n",
    "probabilities = nn.functional.softmax(x, dim=-1)\n",
    "print('\\tprobabilities:', probabilities)\n",
    "## Step 2: select dimension correspoding to target\n",
    "## dimension corresponding to target should have high value, compare to all other dimensions\n",
    "predicted_prob = probabilities[y] \n",
    "print('\\tpredicted_prob:', predicted_prob)\n",
    "## Step 3: negative log likelyhood loss\n",
    "log_prob = -np.log(predicted_prob.tolist())\n",
    "## Step 4: Averaging\n",
    "cross_entropy = log_prob / len(y)\n",
    "print('\\tCross-Entropy:', cross_entropy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7a331e6",
   "metadata": {},
   "source": [
    "# Make Model: Introduces generate() method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b23f4cd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "SKIcLT;AcELMoTbvZv C?nq-QE33:CJqkOKH-q;:la!oiywkHjgChzbQ?u!3bLIgwevmyFJGUGp\n",
      "wnYWmnxKWWev-tDqXErVKLgJ\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "torch.manual_seed(1337)\n",
    "\n",
    "# Check out my Notebook on Embedding Layers: https://www.kaggle.com/code/lunaticsain/docs-embedding-layer-keras\n",
    "\n",
    "class BigramLanguageModel(nn.Module):\n",
    "    def __init__ (self, vocab_size):\n",
    "        super().__init__()\n",
    "        # each token directly reads of the logits for the next token from a lookup table\n",
    "        '''The size of the embedding vector (embedding_dim) for each word or token, in this case is set to vocab_size, \n",
    "            meaning each word or token will be represented by an vocab_size-dimensional embedding vector.\n",
    "        '''\n",
    "        self.token_embedding_table = nn.Embedding(num_embeddings=vocab_size, embedding_dim=vocab_size) # this will make a table of 65x65\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        # idx and targets are both (B, T) of tensor of interger\n",
    "        # each element in idx say 24, will be embedded in the sequence as a 65-dimensional vector\n",
    "        logits = self.token_embedding_table(idx) # (B, T, C) => Batch(batch_size), Time(block_size), Channel(vocab_size)\n",
    "        \n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            # PyTorch cross_entropy expect to be in the shape of B, C\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C) # reshaping it as 2-dimension\n",
    "            targets = targets.view(-1) # reshaping it as single dimension\n",
    "            loss = nn.functional.cross_entropy(logits, targets)\n",
    "        \n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        # idx is a (B, T) array of indices in current context\n",
    "        for _ in range(max_new_tokens):\n",
    "            # get the prediction\n",
    "            logits, loss = self(idx) # forward() # (B, T, C)\n",
    "            # focus only on the last time step i.e., 43, 58, 1, 54 which last val of each time step in xb or idx\n",
    "            ## No matter what we have context we will generate based only on the last char with higher probability\n",
    "            logits = logits[:, -1, :] # becomes (B, C)\n",
    "            # apply softmax to get probabilities\n",
    "            probs = nn.functional.softmax(logits, dim=-1) # (B, C)\n",
    "            # RANDOM SAMPLE from a multinomial distribution with the given probability distribution \n",
    "            ## We are pulling out random stuff, we will have to train the model\n",
    "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
    "            # append sampled index to the running sequence\n",
    "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
    "        return idx\n",
    "\n",
    "m = BigramLanguageModel(vocab_size)\n",
    "# We are giving just 1 char '\\n' (0) and asking it to generate more tokens\n",
    "context = torch.zeros((1,1), dtype=torch.long)\n",
    "print(decode(m.generate(context, max_new_tokens=100)[0].tolist()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "926dd9fa",
   "metadata": {},
   "source": [
    "# Train Model: Optimize loss (Logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a305865a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Pytorch Optimizer\n",
    "optimizer = torch.optim.AdamW(m.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "650449b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.382369041442871\n"
     ]
    }
   ],
   "source": [
    "batch_size = 32\n",
    "block_size = 8\n",
    "\n",
    "for step in range(10000):\n",
    "    # sample a batch of data\n",
    "    xb, yb = get_batch(batch_size, block_size)\n",
    "    \n",
    "    # Evaluate the loss\n",
    "    logits, loss = m.forward(xb, yb)\n",
    "    optimizer.zero_grad(set_to_none=True) # zero out the gradient in the previous step\n",
    "    loss.backward() # Backpropogation to optimize parameters (logits)\n",
    "    optimizer.step()\n",
    "    \n",
    "print(loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e9b4bd7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "lso br. ave aviasurf my, yxMPZI ivee iuedrd whar ksth y h bora s be hese, woweee; the! KI 'de, ulseecherd d o blllando;LUCEO, oraingofof win!\n",
      "RIfans picspeserer hee tha,\n",
      "TOFonk? me ain ckntoty ded. bo'llll st ta d:\n",
      "ELIS me hurf lal y, ma dus pe athouo\n",
      "BEY:! Indy; by s afreanoo adicererupa anse tecorro llaus a!\n",
      "OLeneerithesinthengove fal amas trr\n",
      "TI ar I t, mes, n IUSt my w, fredeeyove\n",
      "THek' merer, dd\n",
      "We ntem lud engitheso; cer ize helorowaginte the?\n",
      "Thak orblyoruldvicee chot, p,\n",
      "Bealivolde Th li\n"
     ]
    }
   ],
   "source": [
    "# We are giving just 1 char '\\n' (0) and asking it to generate more tokens\n",
    "context = torch.zeros((1,1), dtype=torch.long)\n",
    "print(decode(m.generate(context, max_new_tokens=500)[0].tolist()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "311ec82b",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6f2b89d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0: train loss 4.7305, val loss 4.7241\n",
      "step 300: train loss 2.8110, val loss 2.8249\n",
      "step 600: train loss 2.5434, val loss 2.5682\n",
      "step 900: train loss 2.4932, val loss 2.5088\n",
      "step 1200: train loss 2.4863, val loss 2.5035\n",
      "step 1500: train loss 2.4665, val loss 2.4921\n",
      "step 1800: train loss 2.4683, val loss 2.4936\n",
      "step 2100: train loss 2.4696, val loss 2.4846\n",
      "step 2400: train loss 2.4638, val loss 2.4879\n",
      "step 2700: train loss 2.4738, val loss 2.4911\n",
      "\n",
      "MARI he avayokis erceller thour d, myono thishe me tord se by he me, Forder anen: at trselorinjulour t yoru thrd wo ththathy IUShe bavidelanoby man ond be jus as g e atot Meste hrle s, ppat t JLENCOLIUS:\n",
      "Oppid tes d s o ged moer y pevehear soue maramapay fo t: bueyo malalyo!\n",
      "Duir.\n",
      "Fl ke it I t l o'ddre d ondu s?\n",
      "cr, havetrathackes w.\n",
      "PUpee meshancun, hrendspouthoulouren whel's'sesoread pe, s whure our heredinsethes; sedsend r lo pamit,\n",
      "QUMIVIVIOfe m ne RDINid we tr ort; t:\n",
      "MINENXI l dintandore r\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "# hyperparameters\n",
    "batch_size = 32 # how many independent sequences will we process in parallel?\n",
    "block_size = 8 # what is the maximum context length for predictions?\n",
    "max_iters = 3000\n",
    "learning_rate = 1e-2\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu' # if you have gpu, run on it\n",
    "eval_interval = 300\n",
    "eval_iters = 200\n",
    "# ------------\n",
    "\n",
    "torch.manual_seed(1337)\n",
    "\n",
    "# wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
    "with open('input.txt', 'r', encoding='utf-8') as f:\n",
    "    text = f.read()\n",
    "\n",
    "# here are all the unique characters that occur in this text\n",
    "chars = sorted(list(set(text)))\n",
    "vocab_size = len(chars)\n",
    "# create a mapping from characters to integers\n",
    "stoi = { ch:i for i,ch in enumerate(chars) }\n",
    "itos = { i:ch for i,ch in enumerate(chars) }\n",
    "encode = lambda s: [stoi[c] for c in s] # encoder: take a string, output a list of integers\n",
    "decode = lambda l: ''.join([itos[i] for i in l]) # decoder: take a list of integers, output a string\n",
    "\n",
    "# Train and test splits\n",
    "data = torch.tensor(encode(text), dtype=torch.long)\n",
    "n = int(0.9*len(data)) # first 90% will be train, rest val\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]\n",
    "\n",
    "# data loading\n",
    "def get_batch(split):\n",
    "    # generate a small batch of data of inputs x and targets y\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
    "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
    "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
    "    x, y = x.to(device), y.to(device) # sending data to device (cpu or gpu)\n",
    "    return x, y\n",
    "\n",
    "'''\n",
    "    Disabling gradient calculation @torch.no_grad(), is useful for inference,\n",
    "    as we will never gonna call loss.backward() afterwards for gradient calculation.\n",
    "    This will reduce memory consumption for storing the intermediate variable that would have `requires_grad=True`\n",
    "\n",
    "    model.eval() is a kind of switch for some specific layers/parts of the model that \n",
    "    behave differently during training and inference (evaluating) time. \n",
    "    For example, Dropouts Layers, BatchNorm Layers etc., you need to turn them off \n",
    "    during model evaluation, and .eval() will do it for you.\n",
    "    In addition, the common practice for evaluating/validation is using torch.no_grad() \n",
    "    in pair with model.eval() to turn off gradients computation.\n",
    "\n",
    "    ref: https://stackoverflow.com/questions/60018578/what-does-model-eval-do-in-pytorch\n",
    "'''\n",
    "@torch.no_grad()\n",
    "def estimate_loss():\n",
    "    out = {}\n",
    "    '''\n",
    "        Instead of evaluating the model every iter, \n",
    "        we Evaluate every eval_interval based on what the model has been trained so far\n",
    "    '''\n",
    "    model.eval() # Switch to Evaluation mode\n",
    "    for split in ['train', 'val']:\n",
    "        losses = torch.zeros(eval_iters) #making an array of size eval_iters\n",
    "        for k in range(eval_iters):\n",
    "            X, Y = get_batch(split)\n",
    "            logits, loss = model(X, Y)\n",
    "            losses[k] = loss.item()\n",
    "        out[split] = losses.mean()\n",
    "    model.train() # Switch to Training mode\n",
    "    return out\n",
    "\n",
    "# super simple bigram model\n",
    "class BigramLanguageModel(nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_size):\n",
    "        super().__init__()\n",
    "        # each token directly reads off the logits for the next token from a lookup table\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, vocab_size)\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "\n",
    "        # idx and targets are both (B,T) tensor of integers\n",
    "        logits = self.token_embedding_table(idx) # (B,T,C)\n",
    "\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C)\n",
    "            targets = targets.view(B*T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "\n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        # idx is (B, T) array of indices in the current context\n",
    "        for _ in range(max_new_tokens):\n",
    "            # get the predictions\n",
    "            logits, loss = self(idx)\n",
    "            # focus only on the last time step\n",
    "            logits = logits[:, -1, :] # becomes (B, C)\n",
    "            # apply softmax to get probabilities\n",
    "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
    "            # sample from the distribution\n",
    "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
    "            # append sampled index to the running sequence\n",
    "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
    "        return idx\n",
    "\n",
    "# Build Model\n",
    "model = BigramLanguageModel(vocab_size)\n",
    "m = model.to(device)\n",
    "\n",
    "# create a PyTorch optimizer\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Epochs\n",
    "for iter in range(max_iters):\n",
    "    # every once in a while evaluate the loss on train and val sets\n",
    "    # After every eval_interval iters, evaluate the loss on train and val sets\n",
    "    if iter % eval_interval == 0:\n",
    "        losses = estimate_loss()\n",
    "        print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
    "\n",
    "    # sample a batch of data\n",
    "    xb, yb = get_batch('train')\n",
    "\n",
    "    # evaluate the loss\n",
    "    logits, loss = model.forward(xb, yb)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "# generate from the model\n",
    "context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
    "print(decode(m.generate(context, max_new_tokens=500)[0].tolist()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ba53df7",
   "metadata": {},
   "source": [
    "# Mathematical Trick in Self-Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "539aa789",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.1808, -0.0700],\n",
      "        [-0.3596, -0.9152],\n",
      "        [ 0.6258,  0.0255],\n",
      "        [ 0.9545,  0.0643],\n",
      "        [ 0.3612,  1.1679],\n",
      "        [-1.3499, -0.5102],\n",
      "        [ 0.2360, -0.2398],\n",
      "        [-0.9211,  1.5433]])\n",
      "tensor([[ 0.1808, -0.0700],\n",
      "        [-0.0894, -0.4926],\n",
      "        [ 0.1490, -0.3199],\n",
      "        [ 0.3504, -0.2238],\n",
      "        [ 0.3525,  0.0545],\n",
      "        [ 0.0688, -0.0396],\n",
      "        [ 0.0927, -0.0682],\n",
      "        [-0.0341,  0.1332]])\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "    Till now, the tokens do not talk to previous tokens (context)\n",
    "    But we want them to take into account their preceding tokens.\n",
    "    The Easiest way is to take a simple average of the previous tokens (Bag of words)\n",
    "'''\n",
    "# Version 1: using loop\n",
    "torch.manual_seed(1337)\n",
    "B, T, C = 4, 8, 2\n",
    "x = torch.randn(B, T, C) #depicts embedding\n",
    "print(x[0])\n",
    "\n",
    "xbow = torch.zeros((B, T, C))\n",
    "for b in range(B):\n",
    "    for t in range(T):\n",
    "        x_prec = x[b, :t+1]\n",
    "        xbow[b,t] = torch.mean(x_prec, 0)\n",
    "print(xbow[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "05a6839e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a=\n",
      " tensor([[1., 0., 0.],\n",
      "        [1., 1., 0.],\n",
      "        [1., 1., 1.]])\n",
      "b=\n",
      " tensor([[2., 7.],\n",
      "        [6., 4.],\n",
      "        [6., 5.]])\n",
      "c=\n",
      " tensor([[ 2.,  7.],\n",
      "        [ 8., 11.],\n",
      "        [14., 16.]])\n",
      "We can see, it is computing rolling sum\n",
      "------------\n",
      "a=\n",
      " tensor([[1.0000, 0.0000, 0.0000],\n",
      "        [0.5000, 0.5000, 0.0000],\n",
      "        [0.3333, 0.3333, 0.3333]])\n",
      "b=\n",
      " tensor([[2., 7.],\n",
      "        [6., 4.],\n",
      "        [6., 5.]])\n",
      "c=\n",
      " tensor([[2.0000, 7.0000],\n",
      "        [4.0000, 5.5000],\n",
      "        [4.6667, 5.3333]])\n",
      "We can see, it is computing rolling mean\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "    The method is what we want, but the computation is insufficient. Time Complexity: O(B*T*T) (last T is for loop to sum)\n",
    "    Hence we can do smarter computation using matrix multiplication. Time Complexity: O(B^2.81) (accord. strassen algo.)\n",
    "    The example illustrating how matrix multiplication can be used for a \"weighted aggregation\"\n",
    "'''\n",
    "\n",
    "# Aggregation (Sum) of previous values\n",
    "torch.manual_seed(42)\n",
    "a = torch.tril(torch.ones(3, 3))\n",
    "b = torch.randint(0,10,(3,2)).float()\n",
    "c = a @ b\n",
    "print('a=\\n', a)\n",
    "print('b=\\n', b)\n",
    "print('c=\\n', c)\n",
    "print('We can see, it is computing rolling sum')\n",
    "print('------------')\n",
    "\n",
    "# Aggregation (Average) of previous values\n",
    "torch.manual_seed(42)\n",
    "a = torch.tril(torch.ones(3, 3))\n",
    "a = a / torch.sum(a, 1, keepdim=True) # normalizing inorder to make the sum(a) = 1\n",
    "b = torch.randint(0,10,(3,2)).float()\n",
    "c = a @ b\n",
    "print('a=\\n', a)\n",
    "print('b=\\n', b)\n",
    "print('c=\\n', c)\n",
    "print('We can see, it is computing rolling mean')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "02318d65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "isSame: True\n",
      "a=\n",
      " tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.5000, 0.5000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.3333, 0.3333, 0.3333, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.2500, 0.2500, 0.2500, 0.2500, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.2000, 0.2000, 0.2000, 0.2000, 0.2000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.0000, 0.0000],\n",
      "        [0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.0000],\n",
      "        [0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250]])\n",
      "b=\n",
      " tensor([[[ 0.1808, -0.0700],\n",
      "         [-0.3596, -0.9152],\n",
      "         [ 0.6258,  0.0255],\n",
      "         [ 0.9545,  0.0643],\n",
      "         [ 0.3612,  1.1679],\n",
      "         [-1.3499, -0.5102],\n",
      "         [ 0.2360, -0.2398],\n",
      "         [-0.9211,  1.5433]],\n",
      "\n",
      "        [[ 1.3488, -0.1396],\n",
      "         [ 0.2858,  0.9651],\n",
      "         [-2.0371,  0.4931],\n",
      "         [ 1.4870,  0.5910],\n",
      "         [ 0.1260, -1.5627],\n",
      "         [-1.1601, -0.3348],\n",
      "         [ 0.4478, -0.8016],\n",
      "         [ 1.5236,  2.5086]],\n",
      "\n",
      "        [[-0.6631, -0.2513],\n",
      "         [ 1.0101,  0.1215],\n",
      "         [ 0.1584,  1.1340],\n",
      "         [-1.1539, -0.2984],\n",
      "         [-0.5075, -0.9239],\n",
      "         [ 0.5467, -1.4948],\n",
      "         [-1.2057,  0.5718],\n",
      "         [-0.5974, -0.6937]],\n",
      "\n",
      "        [[ 1.6455, -0.8030],\n",
      "         [ 1.3514, -0.2759],\n",
      "         [-1.5108,  2.1048],\n",
      "         [ 2.7630, -1.7465],\n",
      "         [ 1.4516, -1.5103],\n",
      "         [ 0.8212, -0.2115],\n",
      "         [ 0.7789,  1.5333],\n",
      "         [ 1.6097, -0.4032]]])\n",
      "c=\n",
      " tensor([[[ 0.1808, -0.0700],\n",
      "         [-0.0894, -0.4926],\n",
      "         [ 0.1490, -0.3199],\n",
      "         [ 0.3504, -0.2238],\n",
      "         [ 0.3525,  0.0545],\n",
      "         [ 0.0688, -0.0396],\n",
      "         [ 0.0927, -0.0682],\n",
      "         [-0.0341,  0.1332]],\n",
      "\n",
      "        [[ 1.3488, -0.1396],\n",
      "         [ 0.8173,  0.4127],\n",
      "         [-0.1342,  0.4395],\n",
      "         [ 0.2711,  0.4774],\n",
      "         [ 0.2421,  0.0694],\n",
      "         [ 0.0084,  0.0020],\n",
      "         [ 0.0712, -0.1128],\n",
      "         [ 0.2527,  0.2149]],\n",
      "\n",
      "        [[-0.6631, -0.2513],\n",
      "         [ 0.1735, -0.0649],\n",
      "         [ 0.1685,  0.3348],\n",
      "         [-0.1621,  0.1765],\n",
      "         [-0.2312, -0.0436],\n",
      "         [-0.1015, -0.2855],\n",
      "         [-0.2593, -0.1630],\n",
      "         [-0.3015, -0.2293]],\n",
      "\n",
      "        [[ 1.6455, -0.8030],\n",
      "         [ 1.4985, -0.5395],\n",
      "         [ 0.4954,  0.3420],\n",
      "         [ 1.0623, -0.1802],\n",
      "         [ 1.1401, -0.4462],\n",
      "         [ 1.0870, -0.4071],\n",
      "         [ 1.0430, -0.1299],\n",
      "         [ 1.1138, -0.1641]]])\n"
     ]
    }
   ],
   "source": [
    "# Version 2: using MatMul\n",
    "tril = torch.tril(torch.ones(T, T)) # This will not allow past tokens to communicate w/ future tokens\n",
    "wei = tril / torch.sum(tril, 1, keepdim=True)\n",
    "xbow2 = wei @ x # (T, T) @ (B, T, C) ==> (B, T, T) @ (B, T, C) ==> (B, T, C)\n",
    "print('isSame:', torch.allclose(xbow, xbow2))\n",
    "print('a=\\n', wei)\n",
    "print('b=\\n', x)\n",
    "print('c=\\n', xbow2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a2d695c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "isSame: True\n"
     ]
    }
   ],
   "source": [
    "# Version 3: using Softmax\n",
    "tril = torch.tril(torch.ones(T, T)) # This will not allow past tokens to communicate w/ future tokens\n",
    "wei = torch.zeros((T, T))  # The difference is wei here starts with zeros instead of ones -Interaction strength or Affinity\n",
    "wei = wei.masked_fill(tril==0, float('-inf'))\n",
    "wei = F.softmax(wei, dim=-1)\n",
    "xbow3 = wei @ x\n",
    "print('isSame:', torch.allclose(xbow2, xbow3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45bc5d87",
   "metadata": {},
   "source": [
    "# Model - incl. Position Emb. and Linear Transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9ee64a8c",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tkn_emb\n",
      "torch.Size([32, 8, 32])\n",
      "tensor([[-1.7823,  0.1339, -2.0973,  1.9108,  1.6555,  2.0254,  0.6044, -0.7006,\n",
      "          0.8141,  0.2263, -0.8224, -1.1513,  0.1186, -0.3123, -0.6024, -0.1058,\n",
      "         -0.5325,  0.1415, -0.0339, -0.6461,  0.5560, -0.0698, -0.7516, -1.7028,\n",
      "         -0.6811, -1.2044, -0.2007,  1.3154, -0.4974, -0.2338, -0.9047,  0.4135],\n",
      "        [-0.6119, -0.4034,  0.3025,  0.6852, -1.0045, -1.0104, -1.0886,  1.3292,\n",
      "          0.5912, -1.1082, -1.2869, -0.8170,  0.9682,  1.6030, -0.0726, -0.4725,\n",
      "         -1.1616,  0.5962,  1.3058, -0.7422, -1.2529,  0.6750,  1.5664, -0.9238,\n",
      "         -0.0956, -1.5452, -0.1801,  3.1838, -0.1277,  0.0910,  0.5422, -0.6110],\n",
      "        [-0.6631, -0.2513,  1.0101,  0.1215,  0.1584,  1.1340, -1.1539, -0.2984,\n",
      "         -0.5075, -0.9239,  0.5467, -1.4948, -1.2057,  0.5718, -0.5974, -0.6937,\n",
      "          1.6455, -0.8030,  1.3514, -0.2759, -1.5108,  2.1048,  2.7630, -1.7465,\n",
      "          1.4516, -1.5103,  0.8212, -0.2115,  0.7789,  1.5333,  1.6097, -0.4032],\n",
      "        [ 0.1321, -0.5406,  0.1405, -0.7321,  1.1796,  1.3316, -0.2094,  0.0960,\n",
      "          0.9040, -0.4032,  0.3027, -0.8034, -1.2537, -1.5195,  0.7446,  1.1914,\n",
      "         -0.8061, -0.6290,  1.2447, -2.4400,  0.8408, -0.3993, -0.6126, -0.6597,\n",
      "          0.7624,  0.0691,  0.2990, -1.4717,  0.9950,  0.3608,  0.3161,  0.3504],\n",
      "        [-0.0201, -0.2614,  0.9901,  0.6409,  1.2762, -0.7378, -2.2488,  1.2301,\n",
      "          0.3145,  1.8232,  0.0105, -0.3117, -0.2299,  0.0514, -0.0652, -0.0791,\n",
      "         -0.5900,  0.1284,  1.1476,  0.2155,  0.5454, -1.1616, -0.4672,  0.3000,\n",
      "         -0.6073,  0.9019,  0.7696, -0.5458, -0.0453,  0.3953,  1.8599,  1.2386],\n",
      "        [-0.3012, -0.4901, -1.3679,  2.2490,  0.5682,  1.5880, -0.7335, -1.6787,\n",
      "         -0.0336, -1.5213,  0.3886,  1.0050, -1.2381,  1.3319,  0.1538,  0.6376,\n",
      "         -0.7428,  1.6414, -0.2680, -0.4543,  0.7176,  0.3635, -1.1256,  0.9422,\n",
      "          0.8838, -1.9908,  0.8574, -2.1603,  0.3140,  2.2434,  1.6029, -0.7250],\n",
      "        [-0.7393, -0.0373, -0.8158, -2.0201, -0.7839,  0.4154, -0.9601, -1.5174,\n",
      "          0.6209,  0.9996,  0.2797,  0.2285,  0.1394, -1.1975,  0.2783, -1.1303,\n",
      "         -1.3977, -0.7482,  1.6543, -0.1434,  0.5471, -2.1910, -0.7574,  1.9656,\n",
      "          0.9325,  2.3248,  1.5786,  1.2663,  0.8314,  0.6424,  0.9386, -0.7626],\n",
      "        [ 0.4562, -1.0917, -0.8207,  1.8634,  0.8148, -0.0643,  1.4237,  0.2617,\n",
      "         -1.8528,  0.2019, -1.1787, -0.1036, -1.7830, -0.8323, -0.4346, -1.2480,\n",
      "         -0.2880,  0.8809, -0.7190,  0.1745,  0.7520, -0.0629, -0.7111,  0.9810,\n",
      "         -0.7244, -1.5010, -2.8348, -2.8272, -0.1736,  0.0512, -0.6576, -2.5729]],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "pos_emb\n",
      "torch.Size([8, 32])\n",
      "tensor([ 0.7029,  1.4840,  0.1137, -0.7371, -1.7494,  1.6451, -1.1817, -0.6371,\n",
      "        -0.9732, -1.9146,  2.0668,  2.1432,  0.0265,  0.8326, -0.4853, -1.2051,\n",
      "        -0.7008,  2.0997,  0.1142, -0.0647,  0.5901,  0.6097, -0.7080, -0.3850,\n",
      "        -0.7042,  0.0616,  0.0651,  0.7978,  0.6647,  1.0145,  0.4815, -1.5916],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "x\n",
      "torch.Size([32, 8, 32])\n",
      "tensor([[-1.0794,  1.6179, -1.9836,  1.1737, -0.0939,  3.6704, -0.5773, -1.3376,\n",
      "         -0.1591, -1.6883,  1.2444,  0.9919,  0.1451,  0.5203, -1.0878, -1.3108,\n",
      "         -1.2333,  2.2412,  0.0803, -0.7107,  1.1460,  0.5399, -1.4596, -2.0878,\n",
      "         -1.3853, -1.1428, -0.1356,  2.1132,  0.1673,  0.7807, -0.4232, -1.1781],\n",
      "        [-1.5213, -1.6949,  1.1670,  1.4370,  0.3852, -2.7630, -2.2397,  2.2061,\n",
      "          2.7050, -3.2706, -2.0863, -1.4808,  0.0229,  0.9877,  0.0102, -0.8528,\n",
      "         -1.6360,  0.5433,  1.8805, -1.8142, -1.8428,  2.2527,  1.9380,  0.4415,\n",
      "          0.7107, -0.8373,  0.2445,  1.8371, -0.9522, -0.4235,  0.0374,  0.2681],\n",
      "        [-0.2545, -1.7243,  1.8142, -0.0210, -0.7283, -0.0652,  0.2078,  0.8006,\n",
      "         -0.3088, -2.3610,  0.1599, -2.0943, -0.3595,  1.7540,  1.4498, -0.5575,\n",
      "          1.7876, -0.4158,  2.7515, -0.1944,  0.2644,  0.8148,  3.3952, -1.9723,\n",
      "         -0.2768, -1.8322,  0.6675,  1.7780,  0.5952,  0.8440,  2.1458, -0.3955],\n",
      "        [-0.4583, -1.3376, -1.6978,  0.2546,  0.7354,  1.2229, -0.0490,  0.1220,\n",
      "          0.1500, -0.7148,  0.8395, -0.9403, -1.1660, -1.8737,  2.2921,  1.7039,\n",
      "          0.3729, -1.0450,  0.4416, -2.6250,  0.3925, -1.3581, -0.7587, -1.1837,\n",
      "          1.2042, -0.8778,  0.1570, -1.0306,  0.8747,  1.6099, -0.5792,  1.0684],\n",
      "        [ 0.3484,  2.0177,  0.6499, -0.1093,  1.5704,  0.0248,  0.4049,  1.7030,\n",
      "          0.3459,  1.6719,  1.5438,  0.7398, -0.6912,  2.1316,  0.7657, -0.9207,\n",
      "          0.1162, -0.9593,  1.2174, -0.9315, -0.0170, -1.3594,  0.3429, -0.7031,\n",
      "          0.9388,  2.1439,  1.3404, -0.5592, -1.1446,  1.7872,  2.8542,  1.7839],\n",
      "        [-0.2583,  0.4208,  0.1892,  1.5395,  1.2305, -0.0493, -0.1535, -1.7468,\n",
      "          0.2600, -2.6731,  1.9996,  1.5536, -4.0495,  1.4383, -0.9654,  0.6728,\n",
      "         -0.8431,  1.0284,  1.8105,  0.6856,  2.0520,  0.2894, -2.5245,  0.0590,\n",
      "          1.3269, -1.9191,  2.2312, -2.0397,  0.9798,  2.4139,  2.2613,  0.2989],\n",
      "        [-0.2791,  0.2902,  1.3621, -1.2520, -0.8270,  0.5227, -1.1661, -2.7791,\n",
      "          0.3939,  1.1062,  0.1114,  0.4718, -0.4315,  1.1434, -2.2881, -0.1368,\n",
      "         -1.4101, -1.4452,  2.4446,  0.3617, -0.0533, -2.5197, -1.0392,  2.2455,\n",
      "          1.9003,  1.2390,  2.6132,  1.3091,  0.5289,  1.1944,  0.3447,  0.0091],\n",
      "        [ 0.2059, -1.5889, -0.9754,  3.3226,  0.3395,  0.0178,  3.6882, -1.0219,\n",
      "         -2.3067, -0.7403, -1.8483, -0.0433, -1.6366, -0.5317, -1.1301, -2.3425,\n",
      "          0.6415,  1.1964, -1.1132,  0.0777,  2.0580, -1.3246, -0.7781,  1.2809,\n",
      "         -1.4288, -1.5399, -4.7186, -3.4296, -0.5102,  0.3537, -1.0819, -2.3369]],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "logits\n",
      "torch.Size([32, 8, 65])\n",
      "tensor([[-0.5099, -0.9099,  0.3309, -0.4356,  0.7978, -0.1567, -0.7653, -0.1632,\n",
      "         -1.1145,  0.0937,  0.4259, -0.6473, -0.5199,  1.0679, -0.0721,  0.8627,\n",
      "          0.8189, -1.2515, -1.0667, -0.0411,  0.1094,  0.2054, -0.9501, -0.9188,\n",
      "         -1.6277, -0.0491, -0.6050,  0.0872, -0.1733, -0.6862, -0.2364,  1.0540,\n",
      "          0.2788,  2.1429, -0.7776, -0.6963,  0.6242, -0.6952,  1.1931,  1.0426,\n",
      "         -0.7742, -0.0904,  0.0322, -0.5225,  1.6507, -0.6236,  0.0226,  1.5545,\n",
      "          1.3037, -0.4855, -0.4636,  0.8327,  0.0570,  0.1580, -1.2530,  1.3506,\n",
      "          1.4684,  0.1629, -0.5147,  0.6146, -1.4215,  1.2777, -0.1380, -0.3284,\n",
      "          0.5375],\n",
      "        [ 1.1463,  0.8869,  1.1070, -0.1810, -0.3912,  0.8896,  0.2461,  0.3335,\n",
      "         -0.2865, -0.6048, -1.2323, -0.1982,  0.8380,  0.2850, -1.1294,  0.0485,\n",
      "         -0.5154, -0.9962,  0.7600,  1.1976,  1.3642,  0.9918,  1.5131,  0.4566,\n",
      "          0.6228,  0.5746, -0.1782,  0.3132, -0.5686, -0.2749, -0.3303, -0.9892,\n",
      "          2.3184,  0.9812,  1.5905,  0.8616, -0.5526,  0.6980, -0.0632,  0.5395,\n",
      "          0.5567, -1.3541, -0.5479, -0.4732, -0.1983,  0.2495,  0.7792,  0.4324,\n",
      "         -0.2059,  0.4049, -0.2923, -0.0372, -0.2903, -0.2270, -0.7609, -0.2475,\n",
      "          0.0064,  0.6914,  0.9492, -0.9492, -0.0131, -0.4496, -0.1118,  0.1505,\n",
      "         -0.9667],\n",
      "        [ 0.6146,  1.3971,  0.7296,  0.3172,  1.0903, -0.9152,  1.3264, -0.4424,\n",
      "          0.7452, -0.6911, -2.1321, -1.2898,  0.9042,  0.3473, -0.1383,  0.9151,\n",
      "          0.4565,  0.5770,  0.0095,  0.1921,  1.0560, -0.1773, -0.5428,  0.6614,\n",
      "          0.6464, -0.0864,  0.2783,  1.1491, -0.7935, -0.0928, -0.7699, -1.8639,\n",
      "          1.8104,  0.3321,  1.9927, -0.3842, -0.2924, -0.3266, -0.2352,  0.0499,\n",
      "          0.1618, -0.7412,  0.0562,  1.1970,  0.1397,  1.2970,  0.9776,  0.1896,\n",
      "         -0.9422, -1.5561, -1.0729, -0.9793, -0.7873,  0.1597, -1.0638, -1.0112,\n",
      "         -0.5713,  2.0623,  0.4424, -2.1091, -0.4364, -0.0582,  0.5760,  0.8021,\n",
      "          0.4972],\n",
      "        [ 0.9291,  0.5418, -0.6406,  0.9205, -1.8479,  0.1832, -0.0911,  0.8462,\n",
      "          1.0312, -0.4425, -0.2549, -0.0230, -0.0697, -0.9916, -0.7465, -0.4454,\n",
      "          0.7438,  0.0329, -0.3803, -1.1171,  0.4911,  0.0716, -1.5665,  0.6046,\n",
      "          0.2781, -0.1270,  0.1034,  0.7980, -0.1773, -1.0514,  0.2469, -0.1452,\n",
      "          0.1214,  0.8508, -0.7284,  0.2330, -0.3010,  0.2970, -1.1154, -0.1680,\n",
      "          0.2580,  1.4466, -0.4784, -0.1852,  0.2452,  0.3789, -0.4693, -0.1101,\n",
      "          0.3094, -0.2096,  0.1697, -0.0648,  0.5306,  0.7220, -0.9974,  0.4568,\n",
      "         -0.5597,  0.7916, -0.1514, -0.5081, -0.5290,  0.3154, -0.2512,  0.3349,\n",
      "          0.0094],\n",
      "        [-0.1263,  0.8814, -0.7677,  0.3406,  0.7686,  0.0529,  1.4949,  0.7260,\n",
      "          0.1566, -0.1764, -0.0997,  1.5544,  0.3422,  0.6637,  0.4422, -0.7069,\n",
      "          0.5915,  0.1774, -0.7574, -0.0097, -0.3481,  0.0101,  0.2067,  1.0176,\n",
      "          0.2129,  0.3791,  0.2413,  1.1539, -0.3546, -0.7462, -0.5004, -0.2231,\n",
      "          0.8280, -1.2368,  0.3999,  1.0513, -0.5110, -0.1502, -0.6346,  0.4543,\n",
      "         -0.8258,  0.1776,  0.3228,  0.8869, -1.5637,  0.2505,  0.2326, -0.5332,\n",
      "         -1.5591, -0.3043, -0.3690, -1.3355,  0.5388,  0.6295,  0.2358, -0.6433,\n",
      "         -1.3027,  0.9182,  0.1425,  0.9107, -0.3848, -0.7362, -0.5797,  0.5504,\n",
      "         -0.2212],\n",
      "        [ 0.1830, -1.1487, -1.2655,  0.5945,  0.3177, -0.6828,  1.6519, -1.1498,\n",
      "          1.2372,  1.6167,  0.0385,  0.2516, -0.3818,  0.1617,  0.2198,  0.8299,\n",
      "         -0.1490, -0.6250,  0.3035,  0.4966,  0.1019,  0.1585, -0.6752, -0.5262,\n",
      "          1.5755,  0.5986,  1.1029, -0.6420, -0.3946, -1.4690,  1.0410,  0.8737,\n",
      "          0.6105,  1.4499, -0.6927, -0.4987, -0.7169, -0.4240,  0.6338, -1.2891,\n",
      "         -0.6545,  0.2934,  0.3012, -0.8134, -0.5820, -0.1105, -1.4335,  1.0806,\n",
      "         -0.0893, -1.7593, -1.0371, -0.6571, -0.5974,  0.8475, -1.3609,  0.7858,\n",
      "         -0.5973,  0.4480, -1.8563, -0.5496, -0.7952, -0.4951,  0.2866,  1.3181,\n",
      "          0.4965],\n",
      "        [-0.1066,  0.3969, -0.1797, -0.8140,  0.8950,  1.9095,  0.3697,  0.9513,\n",
      "          0.2134,  0.2702,  1.2537,  1.2707,  0.3396,  0.7022,  1.3316,  0.1904,\n",
      "         -0.6928, -1.0433,  0.3285,  0.5354, -1.3326, -0.4378, -0.1400, -0.0593,\n",
      "          0.2617, -0.5013, -0.2886, -0.3669, -0.8956, -1.3163,  1.1889,  0.3648,\n",
      "          0.5991, -0.9022, -0.3940, -1.4251, -1.6735,  0.1465, -0.5978, -0.4429,\n",
      "         -1.3718,  0.2425,  0.7661, -0.2649, -1.0902, -0.9530, -0.4375,  0.1528,\n",
      "         -0.8967,  0.6192, -2.0557,  0.6221, -0.1023, -0.3047,  0.9520,  0.9945,\n",
      "          0.2242,  0.0973,  0.3963,  0.3584,  0.3392, -0.4837, -0.8357, -1.0188,\n",
      "          0.9979],\n",
      "        [-0.8320, -1.8780,  1.0620,  1.2769, -1.2617, -0.5070, -0.7538, -1.1169,\n",
      "         -0.0100,  1.2209, -0.0941, -1.4578, -0.7489, -1.5487,  0.5166,  0.2299,\n",
      "          0.8548, -0.1201,  0.2230,  0.2841,  0.1683, -0.7209, -0.4381, -1.4006,\n",
      "          0.6732, -0.4658,  0.8225, -0.5515,  0.4605,  0.3036,  0.5390, -0.3401,\n",
      "         -0.6272,  1.9415,  0.4883, -1.4386,  1.6806,  0.2757, -0.3288, -0.7854,\n",
      "         -0.0902,  0.6098,  1.0020,  0.3623,  0.1857,  1.0024, -0.3976,  0.1216,\n",
      "          0.7192, -1.4534,  3.2958,  1.1109, -1.0747, -0.6541, -1.0945,  0.2176,\n",
      "          1.5186,  0.0622,  0.2453, -0.2108,  0.9826,  0.8030,  0.0531,  1.6538,\n",
      "         -0.2951]], grad_fn=<SelectBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "# hyperparameters\n",
    "batch_size = 32 # how many independent sequences will we process in parallel?\n",
    "block_size = 8 # what is the maximum context length for predictions?\n",
    "max_iters = 3000\n",
    "learning_rate = 1e-2\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu' # if you have gpu, run on it\n",
    "eval_interval = 300\n",
    "eval_iters = 200\n",
    "n_embd = 32\n",
    "# ------------\n",
    "\n",
    "torch.manual_seed(1337)\n",
    "\n",
    "# wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
    "with open('input.txt', 'r', encoding='utf-8') as f:\n",
    "    text = f.read()\n",
    "\n",
    "# here are all the unique characters that occur in this text\n",
    "chars = sorted(list(set(text)))\n",
    "vocab_size = len(chars)\n",
    "# create a mapping from characters to integers\n",
    "stoi = { ch:i for i,ch in enumerate(chars) }\n",
    "itos = { i:ch for i,ch in enumerate(chars) }\n",
    "encode = lambda s: [stoi[c] for c in s] # encoder: take a string, output a list of integers\n",
    "decode = lambda l: ''.join([itos[i] for i in l]) # decoder: take a list of integers, output a string\n",
    "\n",
    "# Train and test splits\n",
    "data = torch.tensor(encode(text), dtype=torch.long)\n",
    "n = int(0.9*len(data)) # first 90% will be train, rest val\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]\n",
    "\n",
    "# data loading\n",
    "def get_batch(split):\n",
    "    # generate a small batch of data of inputs x and targets y\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
    "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
    "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
    "    x, y = x.to(device), y.to(device) # sending data to device (cpu or gpu)\n",
    "    return x, y\n",
    "\n",
    "@torch.no_grad()\n",
    "def estimate_loss():\n",
    "    out = {}\n",
    "    '''\n",
    "        Instead of evaluating the model every iter, \n",
    "        we Evaluate every eval_interval based on what the model has been trained so far\n",
    "    '''\n",
    "    model.eval() # Switch to Evaluation mode\n",
    "    for split in ['train', 'val']:\n",
    "        losses = torch.zeros(eval_iters)\n",
    "        for k in range(eval_iters):\n",
    "            X, Y = get_batch(split)\n",
    "            logits, loss = model(X, Y)\n",
    "            losses[k] = loss.item()\n",
    "        out[split] = losses.mean()\n",
    "    model.train() # Switch to Training mode\n",
    "    return out\n",
    "\n",
    "# super simple bigram model\n",
    "class BigramLanguageModel(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # Each word or token will be represented by an n_embd-dimensional embedding vector.\n",
    "        self.token_embedding_table = nn.Embedding(num_embeddings=vocab_size, embedding_dim=n_embd)\n",
    "        # We don't just want to encode the identity of token, but also its position\n",
    "        self.position_embedding_table = nn.Embedding(num_embeddings=block_size, embedding_dim=n_embd)\n",
    "        # Applies a linear transformation to the incoming data: :math:`y = x @ W.T + b`\n",
    "        # W will be randomly initialized w/ shape (vocab_size, n_embd) and b w/ shape (vocab_size)\n",
    "        # ref: https://stackoverflow.com/questions/54916135/what-is-the-class-definition-of-nn-linear-in-pytorch\n",
    "        self.lm_head = nn.Linear(in_features=n_embd, out_features=vocab_size)\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "\n",
    "        # idx and targets are both (B,T) tensor of integers\n",
    "        tkn_emb = self.token_embedding_table(idx) # (B,T,C)\n",
    "        print('tkn_emb')\n",
    "        print(tkn_emb.shape)\n",
    "        print(tkn_emb[0])\n",
    "        \n",
    "        # range(0, T):: Every position will have n_embd-dimensional embedding vector.\n",
    "        # Remember the pos_emb for all records in batch will be same\n",
    "        # In real life it is done using sin and cosine functions\n",
    "        pos_emb = self.position_embedding_table(torch.arange(block_size, device=device)) # (T,C)\n",
    "        print('pos_emb')\n",
    "        print(pos_emb.shape)\n",
    "        print(pos_emb[0])\n",
    "        \n",
    "        # x now, not just hold token identity but also its position\n",
    "        x = tkn_emb + pos_emb # (B, T, C)\n",
    "        print('x')\n",
    "        print(x.shape)\n",
    "        print(x[0])\n",
    "        \n",
    "        # Making Logits\n",
    "        logits = self.lm_head(x) # (B,T,vocab_size)\n",
    "        print('logits')\n",
    "        print(logits.shape)\n",
    "        print(logits[0])\n",
    "        \n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C)\n",
    "            targets = targets.view(B*T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "\n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        # idx is (B, T) array of indices in the current context\n",
    "        for _ in range(max_new_tokens):\n",
    "            # get the predictions\n",
    "            logits, loss = self(idx)\n",
    "            # focus only on the last time step\n",
    "            logits = logits[:, -1, :] # becomes (B, C)\n",
    "            # apply softmax to get probabilities\n",
    "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
    "            # sample from the distribution\n",
    "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
    "            # append sampled index to the running sequence\n",
    "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
    "        return idx\n",
    "\n",
    "# Build Model\n",
    "model = BigramLanguageModel()\n",
    "m = model.to(device)\n",
    "\n",
    "# create a PyTorch optimizer\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Epochs\n",
    "for iter in range(max_iters):\n",
    "    # every once in a while evaluate the loss on train and val sets\n",
    "    # After every eval_interval iters, evaluate the loss on train and val sets\n",
    "#     if iter % eval_interval == 0:\n",
    "#         losses = estimate_loss()\n",
    "#         print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
    "\n",
    "    # sample a batch of data\n",
    "    xb, yb = get_batch('train')\n",
    "\n",
    "    # evaluate the loss\n",
    "    logits, loss = model.forward(xb, yb)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    break\n",
    "\n",
    "# # generate from the model\n",
    "# context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
    "# print(decode(m.generate(context, max_new_tokens=500)[0].tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ce4f77d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32]) x containing:\n",
      " tensor([-1.0794,  1.6179, -1.9836,  1.1737, -0.0939,  3.6704, -0.5773, -1.3376,\n",
      "        -0.1591, -1.6883,  1.2444,  0.9919,  0.1451,  0.5203, -1.0878, -1.3108,\n",
      "        -1.2333,  2.2412,  0.0803, -0.7107,  1.1460,  0.5399, -1.4596, -2.0878,\n",
      "        -1.3853, -1.1428, -0.1356,  2.1132,  0.1673,  0.7807, -0.4232, -1.1781]) \n",
      "\n",
      "torch.Size([65, 32]) Parameter containing:\n",
      "tensor([[ 0.1163, -0.1766,  0.0488,  ...,  0.1733, -0.1753, -0.0822],\n",
      "        [ 0.0760,  0.0990, -0.0904,  ..., -0.1290, -0.1246, -0.0028],\n",
      "        [-0.0257,  0.0605, -0.1345,  ..., -0.0785, -0.0084,  0.0847],\n",
      "        ...,\n",
      "        [-0.0027,  0.0686,  0.0374,  ...,  0.0458,  0.1309, -0.1700],\n",
      "        [ 0.0370, -0.1052, -0.1530,  ...,  0.1096,  0.0774, -0.1308],\n",
      "        [-0.0557, -0.0416, -0.0018,  ...,  0.0716, -0.1583, -0.0908]],\n",
      "       requires_grad=True) \n",
      "\n",
      "torch.Size([65]) Parameter containing:\n",
      "tensor([-0.0445,  0.0178,  0.0214, -0.0513, -0.1190, -0.0909,  0.1144, -0.0524,\n",
      "         0.0760, -0.0790,  0.1585,  0.0714,  0.0225,  0.0521, -0.0909,  0.1479,\n",
      "         0.0543, -0.0202,  0.0956, -0.1604, -0.1588,  0.1763, -0.1744, -0.0299,\n",
      "        -0.1020,  0.1132, -0.0107,  0.0141, -0.0216, -0.1228,  0.1099, -0.0983,\n",
      "        -0.0341,  0.0959, -0.1369,  0.0673, -0.1133, -0.1229, -0.0737,  0.1706,\n",
      "        -0.0864,  0.0303,  0.0018, -0.0398,  0.0147, -0.0826, -0.0245,  0.0626,\n",
      "         0.1010, -0.0737,  0.0859, -0.1669, -0.0930,  0.1035,  0.1115,  0.0031,\n",
      "         0.0740, -0.0764,  0.0805,  0.0866,  0.1160,  0.0301,  0.1441, -0.0599,\n",
      "         0.0735], requires_grad=True)\n",
      "--------\n",
      "torch.Size([65]) Linear Transformation:\n",
      "tensor([-0.2141,  0.0633, -0.4226, -0.3542, -1.1928, -0.1402, -1.6711, -0.6026,\n",
      "        -0.0255,  0.2534,  1.1518,  0.0798,  0.9404,  1.1783,  0.4323, -0.7284,\n",
      "         0.3580,  1.1938,  0.4681, -0.3436, -0.8343, -1.0309,  0.0064,  0.9232,\n",
      "        -0.4014, -0.6110, -0.7533, -1.3272, -0.8335, -0.4190, -0.1555, -0.6027,\n",
      "         0.7315, -0.3153,  0.7479,  1.0861, -0.8354,  0.0496, -0.0892, -0.4777,\n",
      "        -1.1028,  0.6934, -0.3498,  0.7455,  1.1644,  0.7222,  0.6471,  0.9241,\n",
      "         1.3202, -0.8549, -0.3317,  0.2037, -0.7322, -0.6787, -0.2904,  1.0554,\n",
      "        -1.2092,  1.9342,  0.6254, -0.4352, -0.7685,  0.1151, -0.7780, -0.4820,\n",
      "         0.9568], grad_fn=<AddBackward0>)\n",
      "torch.Size([65]) Linear Transformation Manual:\n",
      "tensor([-0.2141,  0.0633, -0.4226, -0.3542, -1.1928, -0.1402, -1.6711, -0.6026,\n",
      "        -0.0255,  0.2534,  1.1518,  0.0798,  0.9404,  1.1783,  0.4323, -0.7284,\n",
      "         0.3580,  1.1938,  0.4681, -0.3436, -0.8343, -1.0309,  0.0064,  0.9232,\n",
      "        -0.4014, -0.6110, -0.7533, -1.3272, -0.8335, -0.4190, -0.1555, -0.6027,\n",
      "         0.7315, -0.3153,  0.7479,  1.0861, -0.8354,  0.0496, -0.0892, -0.4777,\n",
      "        -1.1028,  0.6934, -0.3498,  0.7455,  1.1644,  0.7222,  0.6471,  0.9241,\n",
      "         1.3202, -0.8549, -0.3317,  0.2037, -0.7322, -0.6787, -0.2904,  1.0554,\n",
      "        -1.2092,  1.9342,  0.6254, -0.4352, -0.7685,  0.1151, -0.7780, -0.4820,\n",
      "         0.9568], grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# EXTRAS: Explaning nn.Linear -- Linear Transformation\n",
    "''' Linear Transformation is nothing but a normal hidden layer with weights and bias \n",
    "    we can apply any activation function after this, or we may not, it totally on us.\n",
    "'''\n",
    "x = torch.tensor([-1.0794,  1.6179, -1.9836,  1.1737, -0.0939,  3.6704, -0.5773, -1.3376,\n",
    "                 -0.1591, -1.6883,  1.2444,  0.9919,  0.1451,  0.5203, -1.0878, -1.3108,\n",
    "                 -1.2333,  2.2412,  0.0803, -0.7107,  1.1460,  0.5399, -1.4596, -2.0878,\n",
    "                 -1.3853, -1.1428, -0.1356,  2.1132,  0.1673,  0.7807, -0.4232, -1.1781])\n",
    "\n",
    "lt = nn.Linear(32, 65)\n",
    "print(x.shape, 'x containing:\\n', x, '\\n')\n",
    "# Randomly Initialized Parameters\n",
    "print(lt.weight.shape, lt.weight, '\\n')\n",
    "print(lt.bias.shape, lt.bias)\n",
    "print('--------')\n",
    "print(f'{lt(x).shape} Linear Transformation:\\n{lt(x)}')\n",
    "print(f'{(x @ lt.weight.T + lt.bias).shape} Linear Transformation Manual:\\n{x @ lt.weight.T + lt.bias}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8291ec02",
   "metadata": {},
   "source": [
    "# SELF ATTENTION"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2001523",
   "metadata": {},
   "source": [
    "<img src='SelfAttentionHead.png' width=\"1000\" height=\"1000\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "5f61ce7c",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 8, 16])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Version 4: SELF ATTENTION (Decoder Attention Block)\n",
    "torch.manual_seed(1337)\n",
    "B, T, C = 4, 8, 32 # Batch, Time, Channel\n",
    "x = torch.randn(B, T, C) # Depicting Token Embeddings\n",
    "\n",
    "'''\n",
    "Instead of wei being uniform for each prev. token, i.e., have const. affinity for all previous tokens\n",
    "we want it to have self attention (have higher affinity w/ similar tokens)\n",
    "\n",
    "Every single token at each position, emit 2 vectors: Query and Key\n",
    "Query vector: what am I looking for?\n",
    "Key vector: what do I contain?\n",
    "\n",
    "Now the way we get affinity between this tokens is by computing a dot product between keys and Queries\n",
    "\n",
    "A token's Query will be dot product with all the previous token's Keys\n",
    "\n",
    "The tokens whose keys are similar to the token's queries will have high affinity\n",
    "'''\n",
    "\n",
    "# Let's see a single Head perform self-attention\n",
    "head_size = 16 # Huper-param\n",
    "key = nn.Linear(C, head_size, bias=False)\n",
    "query = nn.Linear(C, head_size, bias=False)\n",
    "value = nn.Linear(C, head_size, bias=False)\n",
    "k = key(x) # (B, T, head_size)\n",
    "q = query(x) # (B, T, head_size)\n",
    "v = value(x) # (B, T, head_size)\n",
    "\n",
    "wei = q @ k.transpose(-2, -1) # (B, T, head_size) @ (B, head_size, T) ==> (B, T, T)\n",
    "# print('x\\n', x[0])\n",
    "# print('k\\n', k[0])\n",
    "# print('q\\n', q[0])\n",
    "# print('k.T\\n', k.transpose(-2, -1)[0])\n",
    "# print('wei\\n', wei[0])\n",
    "\n",
    "tril = torch.tril(torch.ones(T, T)) # This will not allow past tokens to communicate w/ future tokens\n",
    "wei = wei.masked_fill(tril==0, float('-inf'))\n",
    "# wei = wei * head_size**-0.5 # Scaled wei; This will prevent explosion of variance due to peak for tokens with higher affinities\n",
    "wei = F.softmax(wei, dim=-1)\n",
    "# xbow4 = wei @ x # (B, T, C)\n",
    "xbow4 = wei @ v # (B, T, head_size)\n",
    "xbow4.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "32e63c30",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.1574, 0.8426, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.2088, 0.1646, 0.6266, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.5792, 0.1187, 0.1889, 0.1131, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.0294, 0.1052, 0.0469, 0.0276, 0.7909, 0.0000, 0.0000, 0.0000],\n",
       "        [0.0176, 0.2689, 0.0215, 0.0089, 0.6812, 0.0019, 0.0000, 0.0000],\n",
       "        [0.1691, 0.4066, 0.0438, 0.0416, 0.1048, 0.2012, 0.0329, 0.0000],\n",
       "        [0.0210, 0.0843, 0.0555, 0.2297, 0.0573, 0.0709, 0.2423, 0.2391]],\n",
       "       grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wei[0] # Notice we don't have const. affinity for all previous tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71b2c2e9",
   "metadata": {},
   "source": [
    "**Inference: In batch 1, last token has this wei: [0.0210, 0.0843, 0.0555, 0.2297, 0.0573, 0.0709, 0.2423, 0.2391] <br>\n",
    "We can see last token (8th) has self-affinity of 0.2391 which is highly close to token 4 with 0.2297, this shows high affinity between token 4th and 8th** \n",
    "<br><br>\n",
    "**If we were to had uniform affinity then the wei would be [0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250], <br>\n",
    "which shows token 8th have same affinity with each previous token**\n",
    "<br><br>\n",
    "**Remember this is just randomly initialized, it has to be optimized through backprop.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "9f0422e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1.8077e-01, -6.9988e-02, -3.5962e-01, -9.1520e-01,  6.2577e-01,\n",
      "          2.5510e-02,  9.5451e-01,  6.4349e-02,  3.6115e-01,  1.1679e+00,\n",
      "         -1.3499e+00, -5.1018e-01,  2.3596e-01, -2.3978e-01, -9.2111e-01,\n",
      "          1.5433e+00,  1.3488e+00, -1.3964e-01,  2.8580e-01,  9.6512e-01,\n",
      "         -2.0371e+00,  4.9314e-01,  1.4870e+00,  5.9103e-01,  1.2603e-01,\n",
      "         -1.5627e+00, -1.1601e+00, -3.3484e-01,  4.4777e-01, -8.0164e-01,\n",
      "          1.5236e+00,  2.5086e+00],\n",
      "        [-6.6310e-01, -2.5128e-01,  1.0101e+00,  1.2155e-01,  1.5840e-01,\n",
      "          1.1340e+00, -1.1539e+00, -2.9840e-01, -5.0754e-01, -9.2392e-01,\n",
      "          5.4671e-01, -1.4948e+00, -1.2057e+00,  5.7182e-01, -5.9735e-01,\n",
      "         -6.9368e-01,  1.6455e+00, -8.0299e-01,  1.3514e+00, -2.7592e-01,\n",
      "         -1.5108e+00,  2.1048e+00,  2.7630e+00, -1.7465e+00,  1.4516e+00,\n",
      "         -1.5103e+00,  8.2115e-01, -2.1153e-01,  7.7890e-01,  1.5333e+00,\n",
      "          1.6097e+00, -4.0323e-01],\n",
      "        [-8.3447e-01,  5.9780e-01, -5.1406e-02, -6.4559e-02, -4.9701e-01,\n",
      "          4.6576e-01, -2.5726e-01, -1.0673e+00,  2.0089e+00, -5.3698e-01,\n",
      "          2.2280e-01,  6.9705e-01, -1.4267e+00,  9.0594e-01,  1.4459e-01,\n",
      "          2.2800e-01,  2.4900e+00, -1.2237e+00,  1.0107e+00,  5.5600e-01,\n",
      "         -1.5935e+00, -1.2706e+00,  6.9033e-01, -1.9614e-01,  3.4491e-01,\n",
      "         -3.4189e-01,  4.7587e-01, -7.6634e-01, -4.1896e-01, -4.3699e-01,\n",
      "         -1.0012e+00, -4.0943e-01],\n",
      "        [-1.6669e+00, -1.3651e+00, -1.6552e-01,  9.6225e-01,  3.1549e-02,\n",
      "         -7.4190e-01, -2.9779e-01,  1.7166e-02, -1.7722e-01, -1.3343e-01,\n",
      "          2.9396e-01,  1.3850e+00,  1.2091e-01,  2.5418e+00, -6.4046e-01,\n",
      "         -1.9740e+00, -3.2957e-01,  7.9589e-03,  9.2623e-01, -1.8846e+00,\n",
      "          1.6696e-01,  4.5862e-01, -1.7662e+00,  5.8599e-01,  1.7510e+00,\n",
      "          2.8072e-01,  3.1096e-01, -6.5376e-01, -6.5763e-01,  3.1845e-01,\n",
      "         -5.4959e-01, -1.4649e+00],\n",
      "        [-2.0555e+00,  1.8275e+00,  1.3035e+00, -4.5013e-01,  1.3471e+00,\n",
      "          1.6910e+00, -1.2445e-01, -1.6824e+00, -2.6608e-02,  7.4049e-02,\n",
      "          1.0517e+00,  6.7789e-01,  3.0665e-01, -7.4723e-01,  7.4349e-01,\n",
      "          8.8766e-01,  2.2874e+00,  9.6114e-01, -1.5297e+00, -2.9122e-01,\n",
      "         -1.1395e-01, -3.1367e-01, -6.2931e-01,  1.1385e+00, -9.9127e-01,\n",
      "          1.6999e-01,  1.2249e+00, -2.3454e-01, -1.0572e+00, -6.5427e-01,\n",
      "          1.5909e+00, -6.9949e-01],\n",
      "        [-8.9606e-01,  6.6191e-02, -5.6280e-02,  2.3412e+00, -2.7234e+00,\n",
      "          5.0967e-01, -8.1447e-01, -2.4604e-01,  4.5085e-03,  2.0474e+00,\n",
      "         -1.5755e-01, -2.1867e-01, -1.3519e+00, -5.7281e-02, -1.8540e+00,\n",
      "         -1.3849e+00, -3.4540e-01, -1.1625e+00,  1.4448e-01,  1.6632e-01,\n",
      "          7.5070e-01,  9.1317e-01, -1.7277e+00,  1.3055e+00,  9.5932e-01,\n",
      "          1.0600e+00,  6.2986e-01, -1.2867e+00, -6.8748e-01,  2.1382e+00,\n",
      "          5.1141e-01,  1.2191e+00],\n",
      "        [ 1.9098e-01, -3.4251e-01,  1.7955e+00,  1.3915e+00,  1.0785e+00,\n",
      "         -6.1495e-01, -4.5885e-01,  5.6748e-01,  1.8289e-02, -1.6608e+00,\n",
      "          1.1169e+00,  5.1965e-01, -1.2423e+00, -9.6182e-01, -8.4998e-02,\n",
      "          1.1854e-01,  2.9843e-01, -7.2636e-01, -3.1187e-01, -4.5604e-01,\n",
      "          6.4407e-01,  6.0728e-01,  1.2397e+00,  7.3249e-01,  5.0418e-01,\n",
      "          8.7135e-01, -2.7416e-01, -7.4689e-01, -5.8324e-01,  3.6988e-01,\n",
      "         -5.5562e-01, -3.9828e-01],\n",
      "        [-5.8188e-01, -2.2083e-01,  1.3537e-02, -3.0574e-01, -3.0384e-02,\n",
      "          8.2161e-01,  3.8670e-04, -4.4742e-01,  8.2040e-01, -1.5178e+00,\n",
      "          6.1587e-01, -1.8648e+00, -9.7773e-01,  6.3224e-02, -4.5483e-01,\n",
      "         -4.1474e-01,  1.4987e+00, -3.9867e-02, -8.0510e-01, -1.1624e+00,\n",
      "          4.2716e-01, -2.8192e-01, -1.2773e-02, -8.7792e-01, -3.2248e-01,\n",
      "          1.8299e-01, -9.3030e-01, -1.2488e+00,  1.1192e+00, -1.9079e+00,\n",
      "         -5.2756e-01,  1.0807e+00]])\n",
      "tensor([[ 0.1808, -0.0700, -0.3596, -0.9152,  0.6258,  0.0255,  0.9545,  0.0643,\n",
      "          0.3612,  1.1679, -1.3499, -0.5102,  0.2360, -0.2398, -0.9211,  1.5433,\n",
      "          1.3488, -0.1396,  0.2858,  0.9651, -2.0371,  0.4931,  1.4870,  0.5910,\n",
      "          0.1260, -1.5627, -1.1601, -0.3348,  0.4478, -0.8016,  1.5236,  2.5086],\n",
      "        [-0.5303, -0.2227,  0.7946, -0.0416,  0.2320,  0.9596, -0.8221, -0.2413,\n",
      "         -0.3708, -0.5947,  0.2482, -1.3398, -0.9788,  0.4441, -0.6483, -0.3416,\n",
      "          1.5988, -0.6986,  1.1837, -0.0806, -1.5937,  1.8511,  2.5621, -1.3786,\n",
      "          1.2430, -1.5185,  0.5093, -0.2309,  0.7268,  1.1658,  1.5962,  0.0550],\n",
      "        [-0.5943,  0.3186,  0.0590, -0.2116, -0.1547,  0.4838, -0.1518, -0.7044,\n",
      "          1.2507, -0.2447, -0.0523,  0.0842, -1.0431,  0.6117, -0.2001,  0.3510,\n",
      "          2.1127, -0.9281,  0.9154,  0.5045, -1.6725, -0.3468,  1.1978, -0.2869,\n",
      "          0.4814, -0.7891,  0.1911, -0.5849, -0.0408, -0.1889, -0.0443,  0.2009],\n",
      "        [-0.3202, -0.1119, -0.1168, -0.4190,  0.2909,  0.1535,  0.3336, -0.1978,\n",
      "          0.5084,  0.4503, -0.6417, -0.1846, -0.2623,  0.3877, -0.6496,  0.6314,\n",
      "          1.4098, -0.4065,  0.6217,  0.4181, -1.6415,  0.3473,  1.1199,  0.1643,\n",
      "          0.5086, -1.1173, -0.4494, -0.4378,  0.1983, -0.3289,  0.8223,  1.1621],\n",
      "        [-1.7753,  1.4074,  1.1197, -0.3466,  1.0781,  1.4589, -0.2120, -1.4098,\n",
      "          0.0256, -0.0332,  0.8682,  0.4349,  0.0590, -0.4253,  0.4873,  0.6308,\n",
      "          2.1297,  0.6144, -0.9864, -0.2569, -0.3791, -0.0592, -0.1797,  0.7411,\n",
      "         -0.5632, -0.0786,  1.0520, -0.2716, -0.7789, -0.3915,  1.4102, -0.5816],\n",
      "        [-1.6099,  1.1770,  1.1506, -0.2784,  0.9556,  1.4617, -0.3880, -1.2484,\n",
      "         -0.1066, -0.1862,  0.8468,  0.0778, -0.1433, -0.3175,  0.3235,  0.4300,\n",
      "          2.0743,  0.4079, -0.6434, -0.2601, -0.5510,  0.3394,  0.3361,  0.3199,\n",
      "         -0.2579, -0.3205,  1.0490, -0.2473, -0.5191, -0.0500,  1.5179, -0.5603],\n",
      "        [-0.7343,  0.0490,  0.5252,  0.4016, -0.2216,  0.7145, -0.5235, -0.3636,\n",
      "         -0.0659,  0.1577,  0.1313, -0.5618, -0.7887,  0.2158, -0.7168, -0.2749,\n",
      "          1.1726, -0.5606,  0.5392, -0.0150, -0.8614,  1.0735,  0.9591, -0.1885,\n",
      "          0.8052, -0.6219,  0.4176, -0.5114,  0.0785,  0.8559,  1.0967,  0.3402],\n",
      "        [-0.7554, -0.3294,  0.5457,  0.6127,  0.1443,  0.1321, -0.3359, -0.1624,\n",
      "          0.2350, -0.7298,  0.5646, -0.0766, -0.7612,  0.4124, -0.4269, -0.5846,\n",
      "          0.7667, -0.3495,  0.0432, -0.7984,  0.0848,  0.3490,  0.0355,  0.1142,\n",
      "          0.6025,  0.2252, -0.0314, -0.8018, -0.0822, -0.0912, -0.1476, -0.1323]],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "tensor([[-0.1571,  0.8801,  0.1615, -0.7824, -0.1429,  0.7468,  0.1007, -0.5239,\n",
      "         -0.8873,  0.1907,  0.1762, -0.5943, -0.4812, -0.4860,  0.2862,  0.5710],\n",
      "        [ 0.6764, -0.5477, -0.2478,  0.3143, -0.1280, -0.2952, -0.4296, -0.1089,\n",
      "         -0.0493,  0.7268,  0.7130, -0.1164,  0.3266,  0.3431, -0.0710,  1.2716],\n",
      "        [ 0.4823, -0.1069, -0.4055,  0.1770,  0.1581, -0.1697,  0.0162,  0.0215,\n",
      "         -0.2490, -0.3773,  0.2787,  0.1629, -0.2895, -0.0676, -0.1416,  1.2194],\n",
      "        [ 0.1971,  0.2856, -0.1303, -0.2655,  0.0668,  0.1954,  0.0281, -0.2451,\n",
      "         -0.4647,  0.0693,  0.1528, -0.2032, -0.2479, -0.1621,  0.1947,  0.7678],\n",
      "        [ 0.2510,  0.7346,  0.5939,  0.2516,  0.2606,  0.7582,  0.5595,  0.3539,\n",
      "         -0.5934, -1.0807, -0.3111, -0.2781, -0.9054,  0.1318, -0.1382,  0.6371],\n",
      "        [ 0.3428,  0.4960,  0.4725,  0.3028,  0.1844,  0.5814,  0.3824,  0.2952,\n",
      "         -0.4897, -0.7705, -0.1172, -0.2541, -0.6892,  0.1979, -0.1513,  0.7666],\n",
      "        [ 0.1866, -0.0964, -0.1430,  0.3059,  0.0834, -0.0069, -0.2047, -0.1535,\n",
      "         -0.0762,  0.3269,  0.3090,  0.0766,  0.0992,  0.1656,  0.1975,  0.7625],\n",
      "        [ 0.1301, -0.0328, -0.4965,  0.2865,  0.2704, -0.2636, -0.0738,  0.3786,\n",
      "          0.0746,  0.0338,  0.0147,  0.3194,  0.2993, -0.1653, -0.0386,  0.3375]],\n",
      "       grad_fn=<SelectBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(x[0])\n",
    "print((wei @ x)[0])\n",
    "print((wei @ v)[0])\n",
    "# Self Attention: (0.1574 * 1.8077e-01) + (0.8426 * -6.6310e-01) = -0.5303\n",
    "# Uniform affinity: (0.5 * 1.8077e-01) + (0.5 * -6.6310e-01) = -0.24"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c564c64",
   "metadata": {},
   "source": [
    "Notes:\n",
    "- Attention is a **communication mechanism**. Can be seen as nodes in a directed graph looking at each other and aggregating information with a weighted sum from all nodes that point to them, with data-dependent weights.\n",
    "<img src='Self_Attention.png' width=\"400\" height=\"400\">\n",
    "- There is no notion of space (position). Attention simply acts over a set of vectors. This is why we need to positionally encode tokens (pos_emb).\n",
    "- Each example across batch dimension is of course processed completely independently and never \"talk\" to each other\n",
    "- In an \"encoder\" attention block just delete the single line that does masking with `tril`, allowing all tokens to communicate (Useful in applications like Sentiment Analysis). This block here is called a \"decoder\" attention block because it has triangular masking, and is usually used in autoregressive settings, like language modeling.\n",
    "- \"self-attention\" just means that the keys and values are produced from the same source as queries (x). In \"cross-attention\", the queries still get produced from x, but the keys and values come from some other, external source (e.g. an encoder module)\n",
    "- \"Scaled\" attention additional divides `wei` by 1/sqrt(head_size). This makes it so when input Q,K are unit variance, wei will be unit variance too and Softmax will stay diffuse and not saturate too much. Illustration below\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d0ae15d",
   "metadata": {},
   "source": [
    "# Model - w/ Self Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "0e1f7b45",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0: train loss 4.2248, val loss 4.2250\n",
      "step 500: train loss 2.6663, val loss 2.6809\n",
      "step 1000: train loss 2.5107, val loss 2.5189\n",
      "step 1500: train loss 2.4394, val loss 2.4447\n",
      "step 2000: train loss 2.3769, val loss 2.3890\n",
      "step 2500: train loss 2.3459, val loss 2.3606\n",
      "step 3000: train loss 2.3163, val loss 2.3361\n",
      "step 3500: train loss 2.2867, val loss 2.3138\n",
      "step 4000: train loss 2.2861, val loss 2.2796\n",
      "step 4500: train loss 2.2692, val loss 2.2816\n",
      "step 4999: train loss 2.2565, val loss 2.2798\n",
      "\n",
      "And'ree aene of wis wit, wild les adery wheerel cromon sace, thincio aldanints wary coutu bus,\n",
      "Whis not hrececthisurouse.\n",
      "\n",
      "BHECLIUS:\n",
      "Mrdtuio bell voenecen sou heant, me sa, ber peth past sor:\n",
      "Andg wour my miven pord, thercave pipes seve onu.\n",
      "\n",
      "Pughe adns wight isen't then, thee my teary, weird, gor sas the DoY: Gas Is.\n",
      "\n",
      "WAnd, hriene thour and ting\n",
      "Pow, your Can wit thatree.\n",
      "\n",
      "That asthe medlect by sail whil the anl cundingaight,\n",
      "Anered whe lan tat aparsted\n",
      "Ang.\n",
      "\n",
      "Fre sthe!\n",
      "Set thr foro Euld fioo in\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "# hyperparameters\n",
    "batch_size = 32 # how many independent sequences will we process in parallel?\n",
    "block_size = 8 # what is the maximum context length for predictions?\n",
    "max_iters = 5000\n",
    "learning_rate = 1e-3 #self-attention can't tolerate high lr\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu' # if you have gpu, run on it\n",
    "eval_interval = 500\n",
    "eval_iters = 200\n",
    "n_embd = 32\n",
    "# ------------\n",
    "\n",
    "torch.manual_seed(1337)\n",
    "\n",
    "# wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
    "with open('input.txt', 'r', encoding='utf-8') as f:\n",
    "    text = f.read()\n",
    "\n",
    "# here are all the unique characters that occur in this text\n",
    "chars = sorted(list(set(text)))\n",
    "vocab_size = len(chars)\n",
    "# create a mapping from characters to integers\n",
    "stoi = { ch:i for i,ch in enumerate(chars) }\n",
    "itos = { i:ch for i,ch in enumerate(chars) }\n",
    "encode = lambda s: [stoi[c] for c in s] # encoder: take a string, output a list of integers\n",
    "decode = lambda l: ''.join([itos[i] for i in l]) # decoder: take a list of integers, output a string\n",
    "\n",
    "# Train and test splits\n",
    "data = torch.tensor(encode(text), dtype=torch.long)\n",
    "n = int(0.9*len(data)) # first 90% will be train, rest val\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]\n",
    "\n",
    "# data loading\n",
    "def get_batch(split):\n",
    "    # generate a small batch of data of inputs x and targets y\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
    "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
    "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
    "    x, y = x.to(device), y.to(device) # sending data to device (cpu or gpu)\n",
    "    return x, y\n",
    "\n",
    "@torch.no_grad()\n",
    "def estimate_loss():\n",
    "    out = {}\n",
    "    '''\n",
    "        Instead of evaluating the model every iter, \n",
    "        we Evaluate every eval_interval based on what the model has been trained so far\n",
    "    '''\n",
    "    model.eval() # Switch to Evaluation mode\n",
    "    for split in ['train', 'val']:\n",
    "        losses = torch.zeros(eval_iters)\n",
    "        for k in range(eval_iters):\n",
    "            X, Y = get_batch(split)\n",
    "            logits, loss = model(X, Y)\n",
    "            losses[k] = loss.item()\n",
    "        out[split] = losses.mean()\n",
    "    model.train() # Switch to Training mode\n",
    "    return out\n",
    "\n",
    "# Self-Attention\n",
    "class Head(nn.Module):\n",
    "    \"\"\" one head of self-attention \"\"\"\n",
    "    \n",
    "    def __init__(self, head_size):\n",
    "        super().__init__()\n",
    "        self.head_size = head_size\n",
    "        self.key = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.query = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.value = nn.Linear(n_embd, head_size, bias=False)        \n",
    "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size))) #tril is not a parameter to be optimized\n",
    "    \n",
    "    def forward(self, x):\n",
    "        B, T, C = x.shape\n",
    "        k = self.key(x)   # (B, T, head_size)\n",
    "        q = self.query(x) # (B, T, head_size)\n",
    "        v = self.value(x) # (B, T, head_size)\n",
    "        \n",
    "        # compute attention scores (\"affinities\")\n",
    "        wei = (q @ k.transpose(-2, -1)) * self.head_size**-0.5   # (B, T, head_size) @ (B, head_size, T) -> (B, T, T)\n",
    "        wei = wei.masked_fill(self.tril[:T, :T]==0, float('-inf'))   # (B, T, T)\n",
    "        wei = F.softmax(wei, dim=-1)  # (B, T, T)\n",
    "        \n",
    "        # perform weighted aggregation of values\n",
    "        out = wei @ v  # (B, T, T) @ (B, T, head_size) -> (B, T, head_size) == (B, T, n_embd)\n",
    "        return out\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\" multiple heads of self-attention in parallel \"\"\"\n",
    "    \n",
    "    def __init__ (self, num_heads, head_size):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return torch.cat([h(x) for h in self.heads], dim=-1) # (B, T, head_size*4) == (B, T, n_embd)\n",
    "    \n",
    "# super simple bigram model\n",
    "class BigramLanguageModel(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # Each word or token will be represented by an n_embd-dimensional embedding vector.\n",
    "        self.token_embedding_table = nn.Embedding(num_embeddings=vocab_size, embedding_dim=n_embd)\n",
    "        # We don't just want to encode the identity of token, but also its position\n",
    "        self.position_embedding_table = nn.Embedding(num_embeddings=block_size, embedding_dim=n_embd)\n",
    "        # Self Attention\n",
    "        ##self.sa_head = Head(head_size = n_embd) # head_size == C, tho' as it is an HP, we can take any value\n",
    "        self.sa_heads = MultiHeadAttention(num_heads=4, head_size = n_embd//4)\n",
    "        # Applies a linear transformation to the incoming data: :math:`y = x @ W.T + b`\n",
    "        self.lm_head = nn.Linear(in_features=n_embd, out_features=vocab_size)\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        B, T = idx.shape\n",
    "        \n",
    "        # idx and targets are both (B,T) tensor of integers\n",
    "        tkn_emb = self.token_embedding_table(idx) # (B,T,C)\n",
    "        \n",
    "        # range(0, T):: Every position will have n_embd-dimensional embedding vector.\n",
    "        pos_emb = self.position_embedding_table(torch.arange(T, device=device)) # (T,C)\n",
    "        \n",
    "        # x now, not just hold token identity but also its position\n",
    "        x = tkn_emb + pos_emb # (B, T, C)\n",
    "        \n",
    "        # self-attention\n",
    "        x = self.sa_heads(x) #forward() # (B, T, head_size)\n",
    "        \n",
    "        # Making Logits\n",
    "        logits = self.lm_head(x) # (B,T,vocab_size)\n",
    "        \n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C)\n",
    "            targets = targets.view(B*T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "\n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        # idx is (B, T) array of indices in the current context\n",
    "        for _ in range(max_new_tokens):\n",
    "            # as we are implementing position embedding (pos_emb), we can't include context more than block_size\n",
    "            # crop idx to the last block_size token\n",
    "            idx_cond = idx[:, -block_size:] # (B, block_size) == (B, T)\n",
    "            # get the predictions\n",
    "            logits, loss = self(idx_cond) #forward()\n",
    "            # focus only on the last time step\n",
    "            logits = logits[:, -1, :] # becomes (B, C)\n",
    "            # apply softmax to get probabilities\n",
    "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
    "            # sample from the distribution\n",
    "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
    "            # append sampled index to the running sequence\n",
    "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
    "        return idx\n",
    "\n",
    "# Build Model\n",
    "model = BigramLanguageModel()\n",
    "m = model.to(device)\n",
    "\n",
    "# create a PyTorch optimizer\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Epochs\n",
    "for iter in range(max_iters):\n",
    "    \n",
    "    # After every eval_interval iters, evaluate the loss on train and val sets\n",
    "    if (iter % eval_interval == 0) or (iter==max_iters-1):\n",
    "        losses = estimate_loss()\n",
    "        print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
    "\n",
    "    # sample a batch of data\n",
    "    xb, yb = get_batch('train')\n",
    "\n",
    "    # train model\n",
    "    logits, loss = model.forward(xb, yb)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "# generate from the model\n",
    "context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
    "print(decode(m.generate(context, max_new_tokens=500)[0].tolist()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e159fa6",
   "metadata": {},
   "source": [
    "# Model - adding Feed Foward Component"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7833eb01",
   "metadata": {},
   "source": [
    "<img src='SelfAttentionModel.png' width=\"200\" height=\"200\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c84a48ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0: train loss 4.2022, val loss 4.2019\n",
      "step 500: train loss 2.6144, val loss 2.6230\n",
      "step 1000: train loss 2.4766, val loss 2.4768\n",
      "step 1500: train loss 2.3985, val loss 2.3938\n",
      "step 2000: train loss 2.3277, val loss 2.3451\n",
      "step 2500: train loss 2.2955, val loss 2.3156\n",
      "step 3000: train loss 2.2825, val loss 2.2921\n",
      "step 3500: train loss 2.2455, val loss 2.2726\n",
      "step 4000: train loss 2.2434, val loss 2.2458\n",
      "step 4500: train loss 2.2293, val loss 2.2418\n",
      "step 4999: train loss 2.2147, val loss 2.2501\n",
      "\n",
      "I farr,\n",
      "Hin of, somord'd yore hit tal det?\n",
      "\n",
      "ghe\n",
      "An ingouf hanto taw.\n",
      "\n",
      "Ichit to learcys als ef sactr, hon cous wore hoin,\n",
      "Mat En, magesrournou wilt with vill ceve sting; ho this, lim dot pruorve I he shu his helly mry hord the the am, fomy therfes arks a bistt he 'ild thes lovey; peavy bre crot mant.\n",
      "\n",
      "FOOKINALANG 'nded nivarll.\n",
      "\n",
      "EY: a tatsuttb my thy-f how play fory tyould to lenl time the sat of ditto it dearrat sate! itt out oumess's lairt, I no prande bourr,\n",
      "Boon. Toth a thime lirt caigeeis sh\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "# hyperparameters\n",
    "batch_size = 32 # how many independent sequences will we process in parallel?\n",
    "block_size = 8 # what is the maximum context length for predictions?\n",
    "max_iters = 5000\n",
    "learning_rate = 1e-3 #self-attention can't tolerate high lr\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu' # if you have gpu, run on it\n",
    "eval_interval = 500\n",
    "eval_iters = 200\n",
    "n_embd = 32\n",
    "# ------------\n",
    "\n",
    "torch.manual_seed(1337)\n",
    "\n",
    "# wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
    "with open('input.txt', 'r', encoding='utf-8') as f:\n",
    "    text = f.read()\n",
    "\n",
    "# here are all the unique characters that occur in this text\n",
    "chars = sorted(list(set(text)))\n",
    "vocab_size = len(chars)\n",
    "# create a mapping from characters to integers\n",
    "stoi = { ch:i for i,ch in enumerate(chars) }\n",
    "itos = { i:ch for i,ch in enumerate(chars) }\n",
    "encode = lambda s: [stoi[c] for c in s] # encoder: take a string, output a list of integers\n",
    "decode = lambda l: ''.join([itos[i] for i in l]) # decoder: take a list of integers, output a string\n",
    "\n",
    "# Train and test splits\n",
    "data = torch.tensor(encode(text), dtype=torch.long)\n",
    "n = int(0.9*len(data)) # first 90% will be train, rest val\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]\n",
    "\n",
    "# data loading\n",
    "def get_batch(split):\n",
    "    # generate a small batch of data of inputs x and targets y\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
    "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
    "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
    "    x, y = x.to(device), y.to(device) # sending data to device (cpu or gpu)\n",
    "    return x, y\n",
    "\n",
    "@torch.no_grad()\n",
    "def estimate_loss():\n",
    "    out = {}\n",
    "    '''\n",
    "        Instead of evaluating the model every iter, \n",
    "        we Evaluate every eval_interval based on what the model has been trained so far\n",
    "    '''\n",
    "    model.eval() # Switch to Evaluation mode\n",
    "    for split in ['train', 'val']:\n",
    "        losses = torch.zeros(eval_iters)\n",
    "        for k in range(eval_iters):\n",
    "            X, Y = get_batch(split)\n",
    "            logits, loss = model(X, Y)\n",
    "            losses[k] = loss.item()\n",
    "        out[split] = losses.mean()\n",
    "    model.train() # Switch to Training mode\n",
    "    return out\n",
    "\n",
    "# Self-Attention\n",
    "class Head(nn.Module):\n",
    "    \"\"\" one head of self-attention \"\"\"\n",
    "    \n",
    "    def __init__(self, head_size):\n",
    "        super().__init__()\n",
    "        self.head_size = head_size\n",
    "        self.key = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.query = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.value = nn.Linear(n_embd, head_size, bias=False)        \n",
    "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size))) #tril is not a parameter to be optimized\n",
    "    \n",
    "    def forward(self, x):\n",
    "        B, T, C = x.shape\n",
    "        k = self.key(x)   # (B, T, head_size)\n",
    "        q = self.query(x) # (B, T, head_size)\n",
    "        v = self.value(x) # (B, T, head_size)\n",
    "        \n",
    "        # compute attention scores (\"affinities\")\n",
    "        wei = (q @ k.transpose(-2, -1)) * self.head_size**-0.5   # (B, T, head_size) @ (B, head_size, T) -> (B, T, T)\n",
    "        wei = wei.masked_fill(self.tril[:T, :T]==0, float('-inf'))   # (B, T, T)\n",
    "        wei = F.softmax(wei, dim=-1)  # (B, T, T)\n",
    "        \n",
    "        # perform weighted aggregation of values\n",
    "        out = wei @ v  # (B, T, T) @ (B, T, head_size) -> (B, T, head_size) == (B, T, n_embd)\n",
    "        return out\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\" multiple heads of self-attention in parallel \"\"\"\n",
    "    \n",
    "    def __init__ (self, num_heads, head_size):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return torch.cat([h(x) for h in self.heads], dim=-1) # (B, T, head_size*4) == (B, T, n_embd)\n",
    "\n",
    "'''\n",
    "Before what we were doing is, just after the self-attention component, we went to directly compute logits.\n",
    "Here we want all tokens now to independently think about the information they gathered by \n",
    "communication w/ other tokens (self-attention).\n",
    "'''\n",
    "class FeedForward(nn.Module):\n",
    "    \"\"\" Simple Linear Layer followed by Non-Linearity\"\"\"\n",
    "    def __init__(self, n_embd):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(n_embd, n_embd),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.net(x)    \n",
    "    \n",
    "# super simple bigram model\n",
    "class BigramLanguageModel(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # Each word or token will be represented by an n_embd-dimensional embedding vector.\n",
    "        self.token_embedding_table = nn.Embedding(num_embeddings=vocab_size, embedding_dim=n_embd)\n",
    "        # We don't just want to encode the identity of token, but also its position\n",
    "        self.position_embedding_table = nn.Embedding(num_embeddings=block_size, embedding_dim=n_embd)\n",
    "        # Self Attention\n",
    "        n_head = 4\n",
    "        self.sa_heads = MultiHeadAttention(num_heads=n_head, head_size = n_embd//n_head)\n",
    "        # Feed Forward\n",
    "        self.ffwd = FeedForward(n_embd)\n",
    "        # Applies a linear transformation to the incoming data: :math:`y = x @ W.T + b`\n",
    "        self.lm_head = nn.Linear(in_features=n_embd, out_features=vocab_size)\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        B, T = idx.shape\n",
    "        \n",
    "        # idx and targets are both (B,T) tensor of integers\n",
    "        tkn_emb = self.token_embedding_table(idx) # (B,T,C)\n",
    "        \n",
    "        # range(0, T):: Every position will have n_embd-dimensional embedding vector.\n",
    "        pos_emb = self.position_embedding_table(torch.arange(T, device=device)) # (T,C)\n",
    "        \n",
    "        # x now, not just hold token identity but also its position\n",
    "        x = tkn_emb + pos_emb # (B, T, C)\n",
    "        \n",
    "        # self-attention\n",
    "        x = self.sa_heads(x) # (B, T, head_size)\n",
    "        \n",
    "        # Feed Forward\n",
    "        x = self.ffwd(x) # (B, T, head_size)\n",
    "        \n",
    "        # Making Logits\n",
    "        logits = self.lm_head(x) # (B,T,vocab_size)\n",
    "        \n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C)\n",
    "            targets = targets.view(B*T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "\n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        # idx is (B, T) array of indices in the current context\n",
    "        for _ in range(max_new_tokens):\n",
    "            # as we are implementing position embedding (pos_emb), we can't include context more than block_size\n",
    "            # crop idx to the last block_size token\n",
    "            idx_cond = idx[:, -block_size:] # (B, block_size) == (B, T)\n",
    "            # get the predictions\n",
    "            logits, loss = self(idx_cond) #forward()\n",
    "            # focus only on the last time step\n",
    "            logits = logits[:, -1, :] # becomes (B, C)\n",
    "            # apply softmax to get probabilities\n",
    "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
    "            # sample from the distribution\n",
    "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
    "            # append sampled index to the running sequence\n",
    "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
    "        return idx\n",
    "\n",
    "# Build Model\n",
    "model = BigramLanguageModel()\n",
    "m = model.to(device)\n",
    "\n",
    "# create a PyTorch optimizer\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Epochs\n",
    "for iter in range(max_iters):\n",
    "    \n",
    "    # After every eval_interval iters, evaluate the loss on train and val sets\n",
    "    if (iter % eval_interval == 0) or (iter==max_iters-1):\n",
    "        losses = estimate_loss()\n",
    "        print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
    "\n",
    "    # sample a batch of data\n",
    "    xb, yb = get_batch('train')\n",
    "\n",
    "    # train model\n",
    "    logits, loss = model.forward(xb, yb)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "# generate from the model\n",
    "context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
    "print(decode(m.generate(context, max_new_tokens=500)[0].tolist()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4b136db",
   "metadata": {},
   "source": [
    "# Model - adding Transformer block Notion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "17e023c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0: train loss 4.2116, val loss 4.2078\n",
      "step 500: train loss 3.1195, val loss 3.1197\n",
      "step 1000: train loss 2.7860, val loss 2.7735\n",
      "step 1500: train loss 2.6227, val loss 2.6083\n",
      "step 2000: train loss 2.5139, val loss 2.5352\n",
      "step 2500: train loss 2.4782, val loss 2.4671\n",
      "step 3000: train loss 2.4179, val loss 2.4335\n",
      "step 3500: train loss 2.3973, val loss 2.4080\n",
      "step 4000: train loss 2.3846, val loss 2.3628\n",
      "step 4500: train loss 2.3617, val loss 2.3660\n",
      "step 4999: train loss 2.3280, val loss 2.3389\n",
      "\n",
      "Hewem opowteeend eth veam this wike-uu nry.\n",
      "\n",
      "LOUTH:\n",
      "Acacy.\n",
      "\n",
      "Tkne Ellisginh ad wo\n",
      "whe moe'n epou:\n",
      "Astt on bno'r feod of pottor Hopyounl, athe of pver longee.\n",
      "Andd too thou no in you go\n",
      "ius ding therbalesous! dilts?\n",
      "\n",
      "Chel, mes hreg cere!\n",
      "Twhaert hert you hou, whowe bok you,\n",
      "Afh het nous eave would grem.\n",
      "RODRUEED:\n",
      "Nes thour\n",
      "Tonluce hou nhe, thes nhas ecetunpent rinl do anle rothy arod thas mavle comfus pibbe'g mripdcale wopuurye?\n",
      "\n",
      "BLis ink sowcume.\n",
      "\n",
      "Paman: do yor:\n",
      "And it thaveoudt: the mas thing bu\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "# hyperparameters\n",
    "batch_size = 32 # how many independent sequences will we process in parallel?\n",
    "block_size = 8 # what is the maximum context length for predictions?\n",
    "max_iters = 5000\n",
    "learning_rate = 1e-3 #self-attention can't tolerate high lr\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu' # if you have gpu, run on it\n",
    "eval_interval = 500\n",
    "eval_iters = 200\n",
    "n_embd = 32\n",
    "# ------------\n",
    "\n",
    "torch.manual_seed(1337)\n",
    "\n",
    "# wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
    "with open('input.txt', 'r', encoding='utf-8') as f:\n",
    "    text = f.read()\n",
    "\n",
    "# here are all the unique characters that occur in this text\n",
    "chars = sorted(list(set(text)))\n",
    "vocab_size = len(chars)\n",
    "# create a mapping from characters to integers\n",
    "stoi = { ch:i for i,ch in enumerate(chars) }\n",
    "itos = { i:ch for i,ch in enumerate(chars) }\n",
    "encode = lambda s: [stoi[c] for c in s] # encoder: take a string, output a list of integers\n",
    "decode = lambda l: ''.join([itos[i] for i in l]) # decoder: take a list of integers, output a string\n",
    "\n",
    "# Train and test splits\n",
    "data = torch.tensor(encode(text), dtype=torch.long)\n",
    "n = int(0.9*len(data)) # first 90% will be train, rest val\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]\n",
    "\n",
    "# data loading\n",
    "def get_batch(split):\n",
    "    # generate a small batch of data of inputs x and targets y\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
    "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
    "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
    "    x, y = x.to(device), y.to(device) # sending data to device (cpu or gpu)\n",
    "    return x, y\n",
    "\n",
    "@torch.no_grad()\n",
    "def estimate_loss():\n",
    "    out = {}\n",
    "    '''\n",
    "        Instead of evaluating the model every iter, \n",
    "        we Evaluate every eval_interval based on what the model has been trained so far\n",
    "    '''\n",
    "    model.eval() # Switch to Evaluation mode\n",
    "    for split in ['train', 'val']:\n",
    "        losses = torch.zeros(eval_iters)\n",
    "        for k in range(eval_iters):\n",
    "            X, Y = get_batch(split)\n",
    "            logits, loss = model(X, Y)\n",
    "            losses[k] = loss.item()\n",
    "        out[split] = losses.mean()\n",
    "    model.train() # Switch to Training mode\n",
    "    return out\n",
    "\n",
    "# Self-Attention\n",
    "class Head(nn.Module):\n",
    "    \"\"\" one head of self-attention \"\"\"\n",
    "    \n",
    "    def __init__(self, head_size):\n",
    "        super().__init__()\n",
    "        self.head_size = head_size\n",
    "        self.key = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.query = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.value = nn.Linear(n_embd, head_size, bias=False)        \n",
    "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size))) #tril is not a parameter to be optimized\n",
    "    \n",
    "    def forward(self, x):\n",
    "        B, T, C = x.shape\n",
    "        k = self.key(x)   # (B, T, head_size)\n",
    "        q = self.query(x) # (B, T, head_size)\n",
    "        v = self.value(x) # (B, T, head_size)\n",
    "        \n",
    "        # compute attention scores (\"affinities\")\n",
    "        wei = (q @ k.transpose(-2, -1)) * self.head_size**-0.5   # (B, T, head_size) @ (B, head_size, T) -> (B, T, T)\n",
    "        wei = wei.masked_fill(self.tril[:T, :T]==0, float('-inf'))   # (B, T, T)\n",
    "        wei = F.softmax(wei, dim=-1)  # (B, T, T)\n",
    "        \n",
    "        # perform weighted aggregation of values\n",
    "        out = wei @ v  # (B, T, T) @ (B, T, head_size) -> (B, T, head_size) == (B, T, n_embd)\n",
    "        return out\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\" multiple heads of self-attention in parallel \"\"\"\n",
    "    \n",
    "    def __init__ (self, num_heads, head_size):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return torch.cat([h(x) for h in self.heads], dim=-1) # (B, T, head_size*4) == (B, T, n_embd)\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    \"\"\" Simple Linear Layer followed by Non-Linearity\"\"\"\n",
    "    def __init__(self, n_embd):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(n_embd, n_embd),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.net(x)    \n",
    "\n",
    "class Block(nn.Module):\n",
    "    \"\"\" Transformer block: communication followed by computation \"\"\"\n",
    "    def __init__(self, n_embd, n_head):\n",
    "        super().__init__()\n",
    "        # n_embd: embedding dimension, n_head: the number of heads we'd like\n",
    "        # Self Attention\n",
    "        head_size = n_embd // n_head\n",
    "        self.sa_heads = MultiHeadAttention(n_head, head_size)\n",
    "        # Feed Forward\n",
    "        self.ffwd = FeedForward(n_embd)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.sa_heads(x)\n",
    "        x = self.ffwd(x)\n",
    "        return x\n",
    "        \n",
    "# super simple bigram model\n",
    "class BigramLanguageModel(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # Each word or token will be represented by an n_embd-dimensional embedding vector.\n",
    "        self.token_embedding_table = nn.Embedding(num_embeddings=vocab_size, embedding_dim=n_embd)\n",
    "        # We don't just want to encode the identity of token, but also its position\n",
    "        self.position_embedding_table = nn.Embedding(num_embeddings=block_size, embedding_dim=n_embd)\n",
    "        # Block Component\n",
    "        self.blocks = nn.Sequential(\n",
    "            Block(n_embd, n_head=4),\n",
    "            Block(n_embd, n_head=4),\n",
    "            Block(n_embd, n_head=4)\n",
    "        )\n",
    "        # Applies a linear transformation to the incoming data: :math:`y = x @ W.T + b`\n",
    "        self.lm_head = nn.Linear(in_features=n_embd, out_features=vocab_size)\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        B, T = idx.shape\n",
    "        \n",
    "        # idx and targets are both (B,T) tensor of integers\n",
    "        tkn_emb = self.token_embedding_table(idx) # (B,T,C)\n",
    "        \n",
    "        # range(0, T):: Every position will have n_embd-dimensional embedding vector.\n",
    "        pos_emb = self.position_embedding_table(torch.arange(T, device=device)) # (T,C)\n",
    "        \n",
    "        # x now, not just hold token identity but also its position\n",
    "        x = tkn_emb + pos_emb # (B, T, C)\n",
    "        \n",
    "        # Block component\n",
    "        x = self.blocks(x) # (B, T, head_size)\n",
    "        \n",
    "        # Making Logits\n",
    "        logits = self.lm_head(x) # (B,T,vocab_size)\n",
    "        \n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C)\n",
    "            targets = targets.view(B*T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "\n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        # idx is (B, T) array of indices in the current context\n",
    "        for _ in range(max_new_tokens):\n",
    "            # as we are implementing position embedding (pos_emb), we can't include context more than block_size\n",
    "            # crop idx to the last block_size token\n",
    "            idx_cond = idx[:, -block_size:] # (B, block_size) == (B, T)\n",
    "            # get the predictions\n",
    "            logits, loss = self(idx_cond) #forward()\n",
    "            # focus only on the last time step\n",
    "            logits = logits[:, -1, :] # becomes (B, C)\n",
    "            # apply softmax to get probabilities\n",
    "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
    "            # sample from the distribution\n",
    "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
    "            # append sampled index to the running sequence\n",
    "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
    "        return idx\n",
    "\n",
    "# Build Model\n",
    "model = BigramLanguageModel()\n",
    "m = model.to(device)\n",
    "\n",
    "# create a PyTorch optimizer\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Epochs\n",
    "for iter in range(max_iters):\n",
    "    \n",
    "    # After every eval_interval iters, evaluate the loss on train and val sets\n",
    "    if (iter % eval_interval == 0) or (iter==max_iters-1):\n",
    "        losses = estimate_loss()\n",
    "        print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
    "\n",
    "    # sample a batch of data\n",
    "    xb, yb = get_batch('train')\n",
    "\n",
    "    # train model\n",
    "    logits, loss = model.forward(xb, yb)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "# generate from the model\n",
    "context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
    "print(decode(m.generate(context, max_new_tokens=500)[0].tolist()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "507515eb",
   "metadata": {},
   "source": [
    "**<span style=\"background-color: #ff9e9e\">This is performing worst than previous one, as model becoming more Deep, it req. more optimitions</span>**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60f600ad",
   "metadata": {},
   "source": [
    "# Model - Skip / Residual connection (Optimization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "f89c4277",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table><tr><td><img src='SelfAttentionModel.png'></td><td><img src='ResidualBlock.png?1'></td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import HTML, display\n",
    "display(HTML(\"<table><tr><td><img src='SelfAttentionModel.png'></td><td><img src='ResidualBlock.png'></td></tr></table>\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0baa573c",
   "metadata": {},
   "source": [
    "- We basically have this Gradient Super-Highway (when backpropogation), that goes directly from the Supervision all the way to the Input unimpeted. <br>\n",
    "- The Gradient also fork off and go through the Residual Blocks. <br>\n",
    "- This Residual blocks are initialized in the beginning in such a way that they contribute very little like it's not even there, and then during the optimization they starts to come online overtime and start to contribute. <br>\n",
    "- In Backpropogation, Addition distributes Gradient equally to both of its branches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "87c3de0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0: train loss 4.6328, val loss 4.6313\n",
      "step 500: train loss 2.3722, val loss 2.3678\n",
      "step 1000: train loss 2.2583, val loss 2.2621\n",
      "step 1500: train loss 2.1740, val loss 2.1979\n",
      "step 2000: train loss 2.1323, val loss 2.1728\n",
      "step 2500: train loss 2.0993, val loss 2.1470\n",
      "step 3000: train loss 2.0636, val loss 2.1339\n",
      "step 3500: train loss 2.0542, val loss 2.1109\n",
      "step 4000: train loss 2.0187, val loss 2.1005\n",
      "step 4500: train loss 1.9986, val loss 2.0940\n",
      "step 4999: train loss 1.9891, val loss 2.0714\n",
      "\n",
      "Time; pall'd that narighoph is sear at forss And thy kingpon my lord, but te stomey, his thou rolder bothis brighile's ther the and in this nep as but as stless, I this,\n",
      "That bry prake with. Velast a fathereeemansshe dyo muthis for have that faker inight seet oben losed on of the lord helle thereace:\n",
      "You sonchark.\n",
      "Be it ry may somen my lives so mualss than negith a chall, oW he be ast youse?\n",
      "\n",
      "NORWARD VIO:\n",
      "In uppare Romeirn leam; and my dade uplebed Jous abse son laudow. doth the how good with me\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "# hyperparameters\n",
    "batch_size = 32 # how many independent sequences will we process in parallel?\n",
    "block_size = 8 # what is the maximum context length for predictions?\n",
    "max_iters = 5000\n",
    "learning_rate = 1e-3 #self-attention can't tolerate high lr\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu' # if you have gpu, run on it\n",
    "eval_interval = 500\n",
    "eval_iters = 200\n",
    "n_embd = 32\n",
    "# ------------\n",
    "\n",
    "torch.manual_seed(1337)\n",
    "\n",
    "# wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
    "with open('input.txt', 'r', encoding='utf-8') as f:\n",
    "    text = f.read()\n",
    "\n",
    "# here are all the unique characters that occur in this text\n",
    "chars = sorted(list(set(text)))\n",
    "vocab_size = len(chars)\n",
    "# create a mapping from characters to integers\n",
    "stoi = { ch:i for i,ch in enumerate(chars) }\n",
    "itos = { i:ch for i,ch in enumerate(chars) }\n",
    "encode = lambda s: [stoi[c] for c in s] # encoder: take a string, output a list of integers\n",
    "decode = lambda l: ''.join([itos[i] for i in l]) # decoder: take a list of integers, output a string\n",
    "\n",
    "# Train and test splits\n",
    "data = torch.tensor(encode(text), dtype=torch.long)\n",
    "n = int(0.9*len(data)) # first 90% will be train, rest val\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]\n",
    "\n",
    "# data loading\n",
    "def get_batch(split):\n",
    "    # generate a small batch of data of inputs x and targets y\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
    "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
    "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
    "    x, y = x.to(device), y.to(device) # sending data to device (cpu or gpu)\n",
    "    return x, y\n",
    "\n",
    "@torch.no_grad()\n",
    "def estimate_loss():\n",
    "    out = {}\n",
    "    '''\n",
    "        Instead of evaluating the model every iter, \n",
    "        we Evaluate every eval_interval based on what the model has been trained so far\n",
    "    '''\n",
    "    model.eval() # Switch to Evaluation mode\n",
    "    for split in ['train', 'val']:\n",
    "        losses = torch.zeros(eval_iters)\n",
    "        for k in range(eval_iters):\n",
    "            X, Y = get_batch(split)\n",
    "            logits, loss = model(X, Y)\n",
    "            losses[k] = loss.item()\n",
    "        out[split] = losses.mean()\n",
    "    model.train() # Switch to Training mode\n",
    "    return out\n",
    "\n",
    "# Self-Attention\n",
    "class Head(nn.Module):\n",
    "    \"\"\" one head of self-attention \"\"\"\n",
    "    \n",
    "    def __init__(self, head_size):\n",
    "        super().__init__()\n",
    "        self.head_size = head_size\n",
    "        self.key = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.query = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.value = nn.Linear(n_embd, head_size, bias=False)        \n",
    "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size))) #tril is not a parameter to be optimized\n",
    "    \n",
    "    def forward(self, x):\n",
    "        B, T, C = x.shape\n",
    "        k = self.key(x)   # (B, T, head_size)\n",
    "        q = self.query(x) # (B, T, head_size)\n",
    "        v = self.value(x) # (B, T, head_size)\n",
    "        \n",
    "        # compute attention scores (\"affinities\")\n",
    "        wei = (q @ k.transpose(-2, -1)) * self.head_size**-0.5   # (B, T, head_size) @ (B, head_size, T) -> (B, T, T)\n",
    "        wei = wei.masked_fill(self.tril[:T, :T]==0, float('-inf'))   # (B, T, T)\n",
    "        wei = F.softmax(wei, dim=-1)  # (B, T, T)\n",
    "        \n",
    "        # perform weighted aggregation of values\n",
    "        out = wei @ v  # (B, T, T) @ (B, T, head_size) -> (B, T, head_size) == (B, T, n_embd)\n",
    "        return out\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\" multiple heads of self-attention in parallel \"\"\"\n",
    "    \n",
    "    def __init__ (self, num_heads, head_size):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
    "        self.proj = nn.Linear(n_embd, n_embd)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = torch.cat([h(x) for h in self.heads], dim=-1) # (B, T, head_size*4) == (B, T, n_embd)\n",
    "        out = self.proj(out) # we are doing Linear transformation of the output from self-attention\n",
    "        return out\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    \"\"\" Simple Linear Layer followed by Non-Linearity\"\"\"\n",
    "    def __init__(self, n_embd):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(n_embd, 4* n_embd), # making ouput of Linear 4 times, as suggested in paper\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4 * n_embd, n_embd) # self.proj # here again making ouput of Linear n_embd, as suggested in paper\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.net(x)    \n",
    "\n",
    "class Block(nn.Module):\n",
    "    \"\"\" Transformer block: communication followed by computation \"\"\"\n",
    "    def __init__(self, n_embd, n_head):\n",
    "        super().__init__()\n",
    "        # n_embd: embedding dimension, n_head: the number of heads we'd like\n",
    "        # Self Attention\n",
    "        head_size = n_embd // n_head\n",
    "        self.sa_heads = MultiHeadAttention(n_head, head_size)\n",
    "        # Feed Forward\n",
    "        self.ffwd = FeedForward(n_embd)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = x + self.sa_heads(x) # we include x and subsiquent fork off, perform some computation and come back.\n",
    "        x = x + self.ffwd(x)\n",
    "        return x\n",
    "        \n",
    "# super simple bigram model\n",
    "class BigramLanguageModel(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # Each word or token will be represented by an n_embd-dimensional embedding vector.\n",
    "        self.token_embedding_table = nn.Embedding(num_embeddings=vocab_size, embedding_dim=n_embd)\n",
    "        # We don't just want to encode the identity of token, but also its position\n",
    "        self.position_embedding_table = nn.Embedding(num_embeddings=block_size, embedding_dim=n_embd)\n",
    "        # Block Component\n",
    "        self.blocks = nn.Sequential(\n",
    "            Block(n_embd, n_head=4),\n",
    "            Block(n_embd, n_head=4),\n",
    "            Block(n_embd, n_head=4)\n",
    "        )\n",
    "        # Applies a linear transformation to the incoming data: :math:`y = x @ W.T + b`\n",
    "        self.lm_head = nn.Linear(in_features=n_embd, out_features=vocab_size)\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        B, T = idx.shape\n",
    "        \n",
    "        # idx and targets are both (B,T) tensor of integers\n",
    "        tkn_emb = self.token_embedding_table(idx) # (B,T,C)\n",
    "        \n",
    "        # range(0, T):: Every position will have n_embd-dimensional embedding vector.\n",
    "        pos_emb = self.position_embedding_table(torch.arange(T, device=device)) # (T,C)\n",
    "        \n",
    "        # x now, not just hold token identity but also its position\n",
    "        x = tkn_emb + pos_emb # (B, T, C)\n",
    "        \n",
    "        # Block component\n",
    "        x = self.blocks(x) # (B, T, head_size)\n",
    "        \n",
    "        # Making Logits\n",
    "        logits = self.lm_head(x) # (B,T,vocab_size)\n",
    "        \n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C)\n",
    "            targets = targets.view(B*T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "\n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        # idx is (B, T) array of indices in the current context\n",
    "        for _ in range(max_new_tokens):\n",
    "            # as we are implementing position embedding (pos_emb), we can't include context more than block_size\n",
    "            # crop idx to the last block_size token\n",
    "            idx_cond = idx[:, -block_size:] # (B, block_size) == (B, T)\n",
    "            # get the predictions\n",
    "            logits, loss = self(idx_cond) #forward()\n",
    "            # focus only on the last time step\n",
    "            logits = logits[:, -1, :] # becomes (B, C)\n",
    "            # apply softmax to get probabilities\n",
    "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
    "            # sample from the distribution\n",
    "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
    "            # append sampled index to the running sequence\n",
    "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
    "        return idx\n",
    "\n",
    "# Build Model\n",
    "model = BigramLanguageModel()\n",
    "m = model.to(device)\n",
    "\n",
    "# create a PyTorch optimizer\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Epochs\n",
    "for iter in range(max_iters):\n",
    "    \n",
    "    # After every eval_interval iters, evaluate the loss on train and val sets\n",
    "    if (iter % eval_interval == 0) or (iter==max_iters-1):\n",
    "        losses = estimate_loss()\n",
    "        print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
    "\n",
    "    # sample a batch of data\n",
    "    xb, yb = get_batch('train')\n",
    "\n",
    "    # train model\n",
    "    logits, loss = model.forward(xb, yb)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "# generate from the model\n",
    "context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
    "print(decode(m.generate(context, max_new_tokens=500)[0].tolist()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db11467e",
   "metadata": {},
   "source": [
    "# Model - LayerNorm (Layer Normalization)\n",
    "\n",
    "Layer Normalization is somewhat similar to Batch Normalization. Though instead of normalizing columns in case of BatchNorm, in LayerNorm we normalize rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "038089de",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0: train loss 4.3103, val loss 4.3100\n",
      "step 500: train loss 2.3804, val loss 2.3800\n",
      "step 1000: train loss 2.2507, val loss 2.2554\n",
      "step 1500: train loss 2.1563, val loss 2.1839\n",
      "step 2000: train loss 2.1193, val loss 2.1598\n",
      "step 2500: train loss 2.0711, val loss 2.1256\n",
      "step 3000: train loss 2.0391, val loss 2.1201\n",
      "step 3500: train loss 2.0331, val loss 2.1010\n",
      "step 4000: train loss 2.0065, val loss 2.0912\n",
      "step 4500: train loss 1.9882, val loss 2.0933\n",
      "step 4999: train loss 1.9758, val loss 2.0636\n",
      "\n",
      "Tito; palastour hen right have seay\n",
      "Of fors sArvows ake meade?'dl as mastede stome cepplace, brother not in brighild'd thee the a lease; for me as but as wolls, brothis,\n",
      "Thele'ty plake abur. I was thing thee.\n",
      "\n",
      "QUORTUS:\n",
      "Morderithem. And my her poft inight seet aben losed on frue aplook a lelp to races; heas batter.\n",
      "\n",
      "GLORCOF MIONNENIUS:\n",
      "I day light alls than neg-to ance kneave he i' all yopsed, and wall I conne anate Roweity leam;\n",
      "And my dade upleble Jous able sinsed.\n",
      "\n",
      "VARICKINTY:\n",
      "Wowll'd it the b\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "# hyperparameters\n",
    "batch_size = 32 # how many independent sequences will we process in parallel?\n",
    "block_size = 8 # what is the maximum context length for predictions?\n",
    "max_iters = 5000\n",
    "learning_rate = 1e-3 #self-attention can't tolerate high lr\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu' # if you have gpu, run on it\n",
    "eval_interval = 500\n",
    "eval_iters = 200\n",
    "n_embd = 32\n",
    "# ------------\n",
    "\n",
    "torch.manual_seed(1337)\n",
    "\n",
    "# wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
    "with open('input.txt', 'r', encoding='utf-8') as f:\n",
    "    text = f.read()\n",
    "\n",
    "# here are all the unique characters that occur in this text\n",
    "chars = sorted(list(set(text)))\n",
    "vocab_size = len(chars)\n",
    "# create a mapping from characters to integers\n",
    "stoi = { ch:i for i,ch in enumerate(chars) }\n",
    "itos = { i:ch for i,ch in enumerate(chars) }\n",
    "encode = lambda s: [stoi[c] for c in s] # encoder: take a string, output a list of integers\n",
    "decode = lambda l: ''.join([itos[i] for i in l]) # decoder: take a list of integers, output a string\n",
    "\n",
    "# Train and test splits\n",
    "data = torch.tensor(encode(text), dtype=torch.long)\n",
    "n = int(0.9*len(data)) # first 90% will be train, rest val\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]\n",
    "\n",
    "# data loading\n",
    "def get_batch(split):\n",
    "    # generate a small batch of data of inputs x and targets y\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
    "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
    "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
    "    x, y = x.to(device), y.to(device) # sending data to device (cpu or gpu)\n",
    "    return x, y\n",
    "\n",
    "@torch.no_grad()\n",
    "def estimate_loss():\n",
    "    out = {}\n",
    "    '''\n",
    "        Instead of evaluating the model every iter, \n",
    "        we Evaluate every eval_interval based on what the model has been trained so far\n",
    "    '''\n",
    "    model.eval() # Switch to Evaluation mode\n",
    "    for split in ['train', 'val']:\n",
    "        losses = torch.zeros(eval_iters)\n",
    "        for k in range(eval_iters):\n",
    "            X, Y = get_batch(split)\n",
    "            logits, loss = model(X, Y)\n",
    "            losses[k] = loss.item()\n",
    "        out[split] = losses.mean()\n",
    "    model.train() # Switch to Training mode\n",
    "    return out\n",
    "\n",
    "# Self-Attention\n",
    "class Head(nn.Module):\n",
    "    \"\"\" one head of self-attention \"\"\"\n",
    "    \n",
    "    def __init__(self, head_size):\n",
    "        super().__init__()\n",
    "        self.head_size = head_size\n",
    "        self.key = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.query = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.value = nn.Linear(n_embd, head_size, bias=False)        \n",
    "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size))) #tril is not a parameter to be optimized\n",
    "    \n",
    "    def forward(self, x):\n",
    "        B, T, C = x.shape\n",
    "        k = self.key(x)   # (B, T, head_size)\n",
    "        q = self.query(x) # (B, T, head_size)\n",
    "        v = self.value(x) # (B, T, head_size)\n",
    "        \n",
    "        # compute attention scores (\"affinities\")\n",
    "        wei = (q @ k.transpose(-2, -1)) * self.head_size**-0.5   # (B, T, head_size) @ (B, head_size, T) -> (B, T, T)\n",
    "        wei = wei.masked_fill(self.tril[:T, :T]==0, float('-inf'))   # (B, T, T)\n",
    "        wei = F.softmax(wei, dim=-1)  # (B, T, T)\n",
    "        \n",
    "        # perform weighted aggregation of values\n",
    "        out = wei @ v  # (B, T, T) @ (B, T, head_size) -> (B, T, head_size) == (B, T, n_embd)\n",
    "        return out\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\" multiple heads of self-attention in parallel \"\"\"\n",
    "    \n",
    "    def __init__ (self, num_heads, head_size):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
    "        self.proj = nn.Linear(n_embd, n_embd)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = torch.cat([h(x) for h in self.heads], dim=-1) # (B, T, head_size*4) == (B, T, n_embd)\n",
    "        out = self.proj(out) # we are doing Linear transformation of the output from self-attention\n",
    "        return out\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    \"\"\" Simple Linear Layer followed by Non-Linearity\"\"\"\n",
    "    def __init__(self, n_embd):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(n_embd, 4* n_embd), # making ouput of Linear 4 times, as suggested in paper\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4 * n_embd, n_embd) # self.proj # here again making ouput of Linear n_embd, as suggested in paper\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.net(x)    \n",
    "\n",
    "class Block(nn.Module):\n",
    "    \"\"\" Transformer block: communication followed by computation \"\"\"\n",
    "    def __init__(self, n_embd, n_head):\n",
    "        super().__init__()\n",
    "        # n_embd: embedding dimension, n_head: the number of heads we'd like\n",
    "        # Self Attention\n",
    "        head_size = n_embd // n_head\n",
    "        self.sa_heads = MultiHeadAttention(n_head, head_size)\n",
    "        # Feed Forward\n",
    "        self.ffwd = FeedForward(n_embd)\n",
    "        # Layer Norms\n",
    "        self.ln1 = nn.LayerNorm(n_embd) # making Unit Gaussians\n",
    "        self.ln2 = nn.LayerNorm(n_embd)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = x + self.sa_heads(self.ln1(x)) # in original paper the layernorm is applied after the computation, though overtime it has become more common to apply it before the computaion\n",
    "        x = x + self.ffwd(self.ln2(x))\n",
    "        return x\n",
    "        \n",
    "# super simple bigram model\n",
    "class BigramLanguageModel(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # Each word or token will be represented by an n_embd-dimensional embedding vector.\n",
    "        self.token_embedding_table = nn.Embedding(num_embeddings=vocab_size, embedding_dim=n_embd)\n",
    "        # We don't just want to encode the identity of token, but also its position\n",
    "        self.position_embedding_table = nn.Embedding(num_embeddings=block_size, embedding_dim=n_embd)\n",
    "        # Block Component\n",
    "        self.blocks = nn.Sequential(\n",
    "            Block(n_embd, n_head=4),\n",
    "            Block(n_embd, n_head=4),\n",
    "            Block(n_embd, n_head=4),\n",
    "            nn.LayerNorm(n_embd)\n",
    "        )\n",
    "        # Applies a linear transformation to the incoming data: :math:`y = x @ W.T + b`\n",
    "        self.lm_head = nn.Linear(in_features=n_embd, out_features=vocab_size)\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        B, T = idx.shape\n",
    "        \n",
    "        # idx and targets are both (B,T) tensor of integers\n",
    "        tkn_emb = self.token_embedding_table(idx) # (B,T,C)\n",
    "        \n",
    "        # range(0, T):: Every position will have n_embd-dimensional embedding vector.\n",
    "        pos_emb = self.position_embedding_table(torch.arange(T, device=device)) # (T,C)\n",
    "        \n",
    "        # x now, not just hold token identity but also its position\n",
    "        x = tkn_emb + pos_emb # (B, T, C)\n",
    "        \n",
    "        # Block component\n",
    "        x = self.blocks(x) # (B, T, head_size)\n",
    "        \n",
    "        # Making Logits\n",
    "        logits = self.lm_head(x) # (B,T,vocab_size)\n",
    "        \n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C)\n",
    "            targets = targets.view(B*T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "\n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        # idx is (B, T) array of indices in the current context\n",
    "        for _ in range(max_new_tokens):\n",
    "            # as we are implementing position embedding (pos_emb), we can't include context more than block_size\n",
    "            # crop idx to the last block_size token\n",
    "            idx_cond = idx[:, -block_size:] # (B, block_size) == (B, T)\n",
    "            # get the predictions\n",
    "            logits, loss = self(idx_cond) #forward()\n",
    "            # focus only on the last time step\n",
    "            logits = logits[:, -1, :] # becomes (B, C)\n",
    "            # apply softmax to get probabilities\n",
    "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
    "            # sample from the distribution\n",
    "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
    "            # append sampled index to the running sequence\n",
    "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
    "        return idx\n",
    "\n",
    "# Build Model\n",
    "model = BigramLanguageModel()\n",
    "m = model.to(device)\n",
    "\n",
    "# create a PyTorch optimizer\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Epochs\n",
    "for iter in range(max_iters):\n",
    "    \n",
    "    # After every eval_interval iters, evaluate the loss on train and val sets\n",
    "    if (iter % eval_interval == 0) or (iter==max_iters-1):\n",
    "        losses = estimate_loss()\n",
    "        print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
    "\n",
    "    # sample a batch of data\n",
    "    xb, yb = get_batch('train')\n",
    "\n",
    "    # train model\n",
    "    logits, loss = model.forward(xb, yb)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "# generate from the model\n",
    "context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
    "print(decode(m.generate(context, max_new_tokens=500)[0].tolist()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68d1d9f2",
   "metadata": {},
   "source": [
    "### EXTRAS: Understanding LayerNorm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "eb718f92",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nx:\\n tensor([[[-1.0940,  0.9237,  1.0306,  ...,  0.2897, -0.5620,  0.3483],\\n         [-1.7736, -0.5168,  0.5872,  ...,  0.2986, -1.6523,  2.2149],\\n         [ 0.5407, -2.0137,  0.9445,  ..., -0.3285,  0.8521,  0.3581],\\n         ...,\\n         [-0.2352,  0.5001,  0.2827,  ..., -0.1093,  0.7275,  1.3204],\\n         [ 2.6609,  0.1081,  2.7206,  ..., -0.0925,  0.4894, -0.0277],\\n         [-0.0695, -0.5672, -0.5143,  ..., -0.4992,  1.0994,  2.7446]],\\n         ....\\n        ])\\n         \\nLayerNorm:\\n tensor([[[-0.7762,  0.7267,  0.8064,  ...,  0.2545, -0.3799,  0.2981],\\n         [-1.2551, -0.3293,  0.4840,  ...,  0.2714, -1.1658,  1.6830],\\n         [ 0.3501, -1.6649,  0.6687,  ..., -0.3355,  0.5958,  0.2061],\\n         ...,\\n         [-0.2336,  0.2989,  0.1414,  ..., -0.1425,  0.4637,  0.8931],\\n         [ 1.7794,  0.1263,  1.8181,  ..., -0.0036,  0.3732,  0.0383],\\n         [-0.0523, -0.4426, -0.4011,  ..., -0.3892,  0.8643,  2.1543]],\\n         ...\\n        ])\\n'"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "''' Batch 1::\n",
    "x:\n",
    " tensor([[-1.0940e+00,  9.2369e-01,  1.0306e+00, -6.9489e-01, -1.7699e+00,\n",
    "          1.8532e+00, -4.6956e-01, -1.3914e+00, -2.6492e+00, -1.2716e+00,\n",
    "          2.1448e+00,  2.7581e+00, -2.4380e-01,  5.7058e-01,  1.1910e+00,\n",
    "         -2.0536e-01, -8.3706e-01,  8.1948e-01, -1.3165e+00, -1.3496e+00,\n",
    "         -3.0246e-01,  1.1609e+00, -1.2607e+00, -1.7108e+00, -8.8963e-01,\n",
    "         -2.5150e-01,  2.1512e-01,  3.2316e+00,  7.1009e-02,  2.8975e-01,\n",
    "         -5.6196e-01,  3.4828e-01],\n",
    "        [-1.7736e+00, -5.1685e-01,  5.8721e-01,  4.8704e-01,  2.6758e+00,\n",
    "         -7.4817e-01, -1.1273e+00,  1.4494e+00,  1.4603e+00, -2.8269e+00,\n",
    "         -5.2299e-01,  1.2928e+00,  3.9777e-01, -4.4793e-01, -1.1879e+00,\n",
    "         -1.4327e+00, -5.6135e-01,  2.8670e-03, -7.5764e-01, -2.0226e+00,\n",
    "         -1.7019e+00,  1.9411e+00,  1.1781e+00,  1.0802e+00,  1.5170e+00,\n",
    "          6.2494e-01,  4.2556e-01, -1.2375e+00, -1.3487e+00,  2.9858e-01,\n",
    "         -1.6523e+00,  2.2149e+00],\n",
    "        [ 5.4068e-01, -2.0137e+00,  9.4452e-01, -8.7465e-01,  2.9296e-01,\n",
    "          1.3235e-01,  1.1524e+00,  1.1950e+00,  1.1027e+00, -1.8403e+00,\n",
    "         -8.4074e-02, -1.4029e+00, -4.0751e-01, -3.3725e-01,  2.7917e+00,\n",
    "          1.3275e+00, -6.6403e-01, -2.4184e-01,  2.6448e+00, -2.3585e+00,\n",
    "          2.6160e+00, -1.6893e+00,  1.9611e-02, -8.8555e-01, -9.6600e-01,\n",
    "         -2.5286e-01,  1.4540e-01,  5.1787e-01,  8.1136e-01, -3.2851e-01,\n",
    "          8.5210e-01,  3.5812e-01],\n",
    "        [-2.8521e+00, -1.4999e+00, -1.9989e+00,  3.2833e+00, -2.3870e+00,\n",
    "          3.4027e-01,  4.4615e-01,  1.1787e+00, -7.8588e-01, -9.6206e-01,\n",
    "          1.3192e+00,  1.1422e+00,  1.0747e+00, -9.2530e-01,  2.7430e+00,\n",
    "          1.6823e+00,  1.3534e+00, -3.7213e-01, -5.1179e-02,  2.2511e-01,\n",
    "         -2.2516e+00, -9.4133e-01,  2.4152e-01,  1.6129e+00,  1.2362e+00,\n",
    "         -1.5004e+00, -1.5125e+00, -5.1243e-01, -1.0272e-02,  1.2794e+00,\n",
    "          7.9406e-01,  2.2734e+00],\n",
    "        [-2.1345e-01,  2.0583e+00, -3.2669e-01, -1.0559e+00,  2.6381e-01,\n",
    "          1.5843e+00,  2.6540e+00,  2.5563e-02,  8.5180e-01, -1.6690e+00,\n",
    "          2.1492e+00, -8.1328e-01, -1.4390e+00,  2.1434e+00,  3.7608e-01,\n",
    "         -1.2563e+00,  2.2049e+00, -1.1276e+00, -7.3530e-01, -2.3094e+00,\n",
    "         -1.3528e-01, -4.7972e-01,  7.9737e-01, -1.8810e+00,  1.2237e+00,\n",
    "          1.4250e+00, -3.5959e-01, -1.2622e+00,  1.9894e-02, -5.1607e-01,\n",
    "          4.6683e-01,  1.6260e+00],\n",
    "        [-2.3517e-01,  5.0012e-01,  2.8267e-01, -1.1373e+00, -1.3541e+00,\n",
    "         -7.9941e-01,  1.7430e+00, -6.9294e-01, -1.4591e+00,  1.1422e+00,\n",
    "          5.6525e-01,  4.8045e-02, -2.5048e+00,  1.5255e+00, -2.1560e+00,\n",
    "         -6.5913e-01,  2.0463e-02,  3.9527e-01,  4.2689e+00,  1.7024e+00,\n",
    "          1.2449e+00, -1.7773e+00, -2.4046e+00,  1.1256e+00,  7.2505e-01,\n",
    "          4.5249e-01,  2.5114e-01, -6.3403e-02,  1.0856e-01, -1.0927e-01,\n",
    "          7.2755e-01,  1.3204e+00],\n",
    "        [ 2.6609e+00,  1.0810e-01,  2.7206e+00,  3.3549e+00, -5.1177e-01,\n",
    "         -2.6488e-02,  1.8399e-01, -1.5500e+00, -1.6921e+00,  1.1674e-01,\n",
    "         -4.6875e-01, -1.3300e+00, -5.5608e-01,  2.2961e+00, -3.1031e+00,\n",
    "          4.7118e-01, -2.3055e-01, -2.8578e+00,  1.5767e+00,  1.1904e+00,\n",
    "         -1.8580e+00,  2.8070e-01, -2.3369e+00, -1.6313e-01,  3.1788e-01,\n",
    "         -1.7727e+00,  1.2914e+00, -1.2241e+00, -3.8053e-02, -9.2530e-02,\n",
    "          4.8940e-01, -2.7709e-02],\n",
    "        [-6.9476e-02, -5.6724e-01, -5.1432e-01,  5.4404e-01,  1.5037e-01,\n",
    "          1.0765e-01,  3.2190e+00, -1.2193e+00, -9.2795e-02,  2.2568e-01,\n",
    "         -2.0195e+00, -4.4993e-01,  3.8238e-01,  6.0903e-02, -1.6166e+00,\n",
    "          4.4886e-01,  2.2783e+00,  1.7588e-01, -1.0842e-01,  8.6834e-01,\n",
    "         -7.3116e-01, -7.6861e-01,  1.4200e+00,  8.9089e-01, -5.7830e-01,\n",
    "         -1.6016e+00, -3.0439e+00, -9.3725e-01,  1.1119e-01, -4.9917e-01,\n",
    "          1.0994e+00,  2.7446e+00]])\n",
    "x After LayerNorm:\n",
    " tensor([[-0.7762,  0.7267,  0.8064, -0.4789, -1.2797,  1.4191, -0.3111, -0.9977,\n",
    "         -1.9347, -0.9085,  1.6363,  2.0932, -0.1429,  0.4637,  0.9258, -0.1143,\n",
    "         -0.5848,  0.6491, -0.9420, -0.9666, -0.1866,  0.9034, -0.9004, -1.2357,\n",
    "         -0.6240, -0.1487,  0.1989,  2.4459,  0.0916,  0.2545, -0.3799,  0.2981],\n",
    "        [-1.2551, -0.3293,  0.4840,  0.4102,  2.0225, -0.4997, -0.7790,  1.1191,\n",
    "          1.1271, -2.0310, -0.3339,  1.0038,  0.3444, -0.2786, -0.8237, -1.0040,\n",
    "         -0.3621,  0.0535, -0.5067, -1.4385, -1.2023,  1.4813,  0.9192,  0.8471,\n",
    "          1.1689,  0.5118,  0.3649, -0.8602, -0.9421,  0.2714, -1.1658,  1.6830],\n",
    "        [ 0.3501, -1.6649,  0.6687, -0.7663,  0.1547,  0.0280,  0.8327,  0.8663,\n",
    "          0.7935, -1.5281, -0.1427, -1.1831, -0.3978, -0.3424,  2.1259,  0.9708,\n",
    "         -0.6002, -0.2671,  2.0100, -1.9369,  1.9873, -1.4090, -0.0609, -0.7749,\n",
    "         -0.8384, -0.2758,  0.0383,  0.3321,  0.5637, -0.3355,  0.5958,  0.2061],\n",
    "        [-1.9574, -1.0652, -1.3944,  2.0909, -1.6506,  0.1490,  0.2189,  0.7022,\n",
    "         -0.5941, -0.7103,  0.7949,  0.6781,  0.6336, -0.6861,  1.7344,  1.0345,\n",
    "          0.8175, -0.3211, -0.1093,  0.0730, -1.5612, -0.6966,  0.0838,  0.9887,\n",
    "          0.7401, -1.0655, -1.0735, -0.4136, -0.0823,  0.7686,  0.4484,  1.4246],\n",
    "        [-0.2614,  1.4477, -0.3466, -0.8952,  0.0976,  1.0910,  1.8958, -0.0816,\n",
    "          0.5400, -1.3565,  1.5160, -0.7127, -1.1835,  1.5117,  0.1821, -1.0460,\n",
    "          1.5579, -0.9492, -0.6540, -1.8383, -0.2026, -0.4618,  0.4990, -1.5160,\n",
    "          0.8197,  0.9712, -0.3714, -1.0505, -0.0859, -0.4891,  0.2503,  1.1224],\n",
    "        [-0.2336,  0.2989,  0.1414, -0.8871, -1.0441, -0.6423,  1.1992, -0.5652,\n",
    "         -1.1202,  0.7640,  0.3461, -0.0285, -1.8776,  1.0416, -1.6249, -0.5407,\n",
    "         -0.0485,  0.2230,  3.0287,  1.1698,  0.8384, -1.3506, -1.8050,  0.7520,\n",
    "          0.4619,  0.2644,  0.1186, -0.1092,  0.0153, -0.1425,  0.4637,  0.8931],\n",
    "        [ 1.7794,  0.1263,  1.8181,  2.2288, -0.2751,  0.0391,  0.1754, -0.9475,\n",
    "         -1.0395,  0.1319, -0.2473, -0.8050, -0.3038,  1.5432, -1.9532,  0.3614,\n",
    "         -0.0930, -1.7944,  1.0773,  0.8272, -1.1469,  0.2381, -1.4571, -0.0494,\n",
    "          0.2621, -1.0917,  0.8925, -0.7364,  0.0316, -0.0036,  0.3732,  0.0383],\n",
    "        [-0.0523, -0.4426, -0.4011,  0.4288,  0.1201,  0.0866,  2.5263, -0.9539,\n",
    "         -0.0706,  0.1792, -1.5814, -0.3506,  0.3020,  0.0500, -1.2654,  0.3542,\n",
    "          1.7887,  0.1401, -0.0828,  0.6831, -0.5711, -0.6005,  1.1157,  0.7008,\n",
    "         -0.4513, -1.2537, -2.3846, -0.7327,  0.0894, -0.3892,  0.8643,  2.1543]])\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "a3612157",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LayerNorm Variantion: 1.0\n",
      "LayeNorm Mean: 0.0\n"
     ]
    }
   ],
   "source": [
    "x_batch1_token1_embd = [-0.7762,  0.7267,  0.8064, -0.4789, -1.2797,  1.4191, -0.3111, -0.9977,\n",
    "         -1.9347, -0.9085,  1.6363,  2.0932, -0.1429,  0.4637,  0.9258, -0.1143,\n",
    "         -0.5848,  0.6491, -0.9420, -0.9666, -0.1866,  0.9034, -0.9004, -1.2357,\n",
    "         -0.6240, -0.1487,  0.1989,  2.4459,  0.0916,  0.2545, -0.3799,  0.2981]\n",
    "\n",
    "print('LayerNorm Variantion:', np.round(np.var(x_batch1_token1_embd), 2))\n",
    "print('LayeNorm Mean:', np.round(np.mean(x_batch1_token1_embd), 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "b3f54783",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x: torch.Size([32, 100])\n",
      " tensor([[ 0.1808, -0.0700, -0.3596,  ..., -1.3651, -0.1655,  0.9623],\n",
      "        [ 0.0315, -0.7419, -0.2978,  ..., -0.6150, -0.4589,  0.5675],\n",
      "        [ 0.0183, -1.6608,  1.1169,  ..., -0.9001,  0.6614,  0.5118],\n",
      "        ...,\n",
      "        [-1.6462, -1.6728, -0.7227,  ..., -0.9647,  0.1162, -0.8295],\n",
      "        [-0.2266,  0.0219, -0.2785,  ..., -0.9094, -1.3062, -0.7847],\n",
      "        [ 0.1956, -0.2808, -0.5215,  ..., -1.6868, -0.9292, -1.2395]]) \n",
      "\n",
      "\n",
      "BatchNorm: tensor([[ 0.1392, -0.5709, -0.3944,  0.0442, -0.1492,  0.0062, -0.2630,  0.4211,\n",
      "          0.1554,  0.1999,  0.2484, -0.2488, -0.0572, -0.0869,  0.1383,  0.2357,\n",
      "          0.2467,  0.1547, -0.2061, -0.3572,  0.2785,  0.2085, -0.2316, -0.0536,\n",
      "          0.1361, -0.1408,  0.1386, -0.2578,  0.1453,  0.0186,  0.0714, -0.2406,\n",
      "         -0.0831,  0.1026,  0.1838,  0.0831, -0.0111,  0.3121,  0.1191, -0.2944,\n",
      "          0.0761,  0.1658,  0.0705,  0.1351,  0.0082, -0.2185, -0.0326, -0.0873,\n",
      "          0.1913, -0.0268, -0.1935, -0.4532, -0.1081,  0.0459,  0.0470,  0.1614,\n",
      "         -0.3075, -0.3038,  0.2629, -0.1148,  0.0347,  0.2275,  0.2808,  0.2773,\n",
      "         -0.1686,  0.2929, -0.1962,  0.0880, -0.1901,  0.2263,  0.1448,  0.1242,\n",
      "          0.1586,  0.1507, -0.0147, -0.0184, -0.0025,  0.0436,  0.0262,  0.1105,\n",
      "          0.0145,  0.0791, -0.0398,  0.0273, -0.2425, -0.0481, -0.1501, -0.1037,\n",
      "         -0.0941,  0.1481,  0.1545,  0.0372,  0.1804, -0.1994, -0.1312,  0.1699,\n",
      "         -0.1604, -0.2711,  0.1440, -0.1964]]) \n",
      " tensor([[0.7920, 0.8402, 0.8645, 0.5862, 0.7258, 0.7946, 0.7983, 1.1099, 0.7741,\n",
      "         1.0801, 1.0839, 1.1949, 0.8045, 0.7455, 1.5089, 0.9587, 1.0271, 0.9053,\n",
      "         1.1584, 1.2895, 1.2129, 1.2039, 1.5756, 1.2348, 0.6094, 0.9059, 0.8112,\n",
      "         0.7709, 0.7342, 0.9420, 0.6977, 0.8983, 1.0756, 0.9665, 1.1524, 1.1679,\n",
      "         0.9480, 1.5385, 0.7262, 0.6268, 1.4609, 0.9662, 1.0875, 1.1557, 1.3902,\n",
      "         0.6700, 0.6511, 0.7912, 0.6843, 1.3611, 1.4164, 1.0427, 1.1643, 0.9569,\n",
      "         1.4022, 1.3151, 0.8261, 1.2287, 0.8325, 0.9921, 0.8062, 0.9197, 0.9164,\n",
      "         0.8429, 1.1065, 1.1417, 1.3588, 0.8875, 0.7902, 0.9980, 0.9928, 1.0374,\n",
      "         0.8345, 0.9112, 0.9669, 0.7708, 1.2054, 0.6723, 0.9020, 1.3105, 1.2923,\n",
      "         1.3466, 1.0578, 1.8982, 0.7544, 0.8949, 1.1824, 1.0899, 0.7906, 0.9492,\n",
      "         1.1165, 0.7706, 1.0321, 0.8883, 0.6412, 0.9758, 0.9283, 1.3117, 0.7578,\n",
      "         0.8580]])\n",
      "-------\n",
      "LayerNorm: tensor([[ 0.0409],\n",
      "        [ 0.0701],\n",
      "        [-0.2667],\n",
      "        [-0.0407],\n",
      "        [-0.1346],\n",
      "        [-0.0208],\n",
      "        [-0.1020],\n",
      "        [-0.0780],\n",
      "        [ 0.0502],\n",
      "        [ 0.1726],\n",
      "        [-0.1185],\n",
      "        [ 0.0410],\n",
      "        [-0.0751],\n",
      "        [-0.0210],\n",
      "        [ 0.2044],\n",
      "        [ 0.0298],\n",
      "        [ 0.0182],\n",
      "        [ 0.0702],\n",
      "        [-0.0514],\n",
      "        [-0.2094],\n",
      "        [ 0.0603],\n",
      "        [-0.0183],\n",
      "        [ 0.2128],\n",
      "        [-0.0231],\n",
      "        [ 0.0253],\n",
      "        [ 0.0168],\n",
      "        [ 0.0518],\n",
      "        [-0.0567],\n",
      "        [ 0.2676],\n",
      "        [ 0.1119],\n",
      "        [ 0.0148],\n",
      "        [-0.1235]]) \n",
      " tensor([[1.0974],\n",
      "        [1.1914],\n",
      "        [0.8626],\n",
      "        [1.0505],\n",
      "        [1.2440],\n",
      "        [0.9635],\n",
      "        [1.2625],\n",
      "        [0.9310],\n",
      "        [1.2030],\n",
      "        [0.9291],\n",
      "        [0.9190],\n",
      "        [1.1037],\n",
      "        [0.8750],\n",
      "        [1.0331],\n",
      "        [0.9004],\n",
      "        [0.8774],\n",
      "        [0.9265],\n",
      "        [0.7519],\n",
      "        [0.9805],\n",
      "        [0.8804],\n",
      "        [1.1778],\n",
      "        [0.8729],\n",
      "        [0.9903],\n",
      "        [0.7439],\n",
      "        [0.9101],\n",
      "        [0.9875],\n",
      "        [0.9775],\n",
      "        [0.7879],\n",
      "        [1.2659],\n",
      "        [1.1954],\n",
      "        [1.1272],\n",
      "        [1.0917]])\n",
      "tensor([[1.0974],\n",
      "        [1.1914],\n",
      "        [0.8626],\n",
      "        [1.0505],\n",
      "        [1.2440],\n",
      "        [0.9635],\n",
      "        [1.2625],\n",
      "        [0.9310],\n",
      "        [1.2030],\n",
      "        [0.9291],\n",
      "        [0.9190],\n",
      "        [1.1037],\n",
      "        [0.8750],\n",
      "        [1.0331],\n",
      "        [0.9004],\n",
      "        [0.8774],\n",
      "        [0.9265],\n",
      "        [0.7519],\n",
      "        [0.9805],\n",
      "        [0.8804],\n",
      "        [1.1778],\n",
      "        [0.8729],\n",
      "        [0.9903],\n",
      "        [0.7439],\n",
      "        [0.9101],\n",
      "        [0.9875],\n",
      "        [0.9775],\n",
      "        [0.7879],\n",
      "        [1.2659],\n",
      "        [1.1954],\n",
      "        [1.1272],\n",
      "        [1.0917]])\n"
     ]
    }
   ],
   "source": [
    "# BatchNorm v LayerNorm\n",
    "class layerNorm():\n",
    "    def __init__ (self, dim, eps=1e-5):\n",
    "        self.eps = eps\n",
    "        self.gamma =  torch.ones(dim)\n",
    "        self.beta = torch.zeros(dim)\n",
    "        \n",
    "    def __call__ (self, x):\n",
    "        # calculate the forward pass\n",
    "        '''\n",
    "            when taken mean,var along dim=0, i.e., of every feature across all inputs, it is BatchNorm.\n",
    "            and if mean,var along dim=1, i.e., of the same feature, it is layerNorm.\n",
    "        '''\n",
    "        x_mean = x.mean(dim=1, keepdim=True)\n",
    "        x_var = x.var(dim=1, keepdim=True)\n",
    "        print(\"BatchNorm:\", x.mean(dim=0, keepdim=True), '\\n', x.var(dim=0, keepdim=True))\n",
    "        print('-------')\n",
    "        print(\"LayerNorm:\", x.mean(dim=1, keepdim=True), '\\n', x.var(dim=1, keepdim=True))\n",
    "        print(x_var)\n",
    "        xhat = (x - x_mean) / torch.sqrt(x_var + self.eps) #nornatize to unit variance\n",
    "        self.out = self.gamma * xhat + self.beta\n",
    "        return self.out\n",
    "    \n",
    "    def parneters(sett):\n",
    "        return (self.gamma, self.beta)\n",
    "                \n",
    "torch.manual_seed(1337)\n",
    "x = torch.randn(32, 100) #batch size of 32 of 100 dimensional vectors\n",
    "print(f'x: {x.shape}\\n', x, '\\n\\n')\n",
    "module = layerNorm(100)\n",
    "x = module(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "3d920065",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BatchNorm: tensor(0.1469) tensor(0.8803)\n",
      "LayerNorm: tensor(-9.5367e-09) tensor(1.0000)\n"
     ]
    }
   ],
   "source": [
    "# It will return Gaussian mean and std\n",
    "print(\"BatchNorm:\", x[:,0].mean(), x[:,0].std()) # mean, std of every feature across all inputs\n",
    "print(\"LayerNorm:\", x[0,:].mean(), x[0,:].std()) # mean, std of single input from the batch, of its features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "485b6f93",
   "metadata": {},
   "source": [
    "# Final Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "59f80109",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_14284\\730405776.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    221\u001b[0m     \u001b[1;31m# After every eval_interval iters, evaluate the loss on train and val sets\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    222\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0miter\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0meval_interval\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0miter\u001b[0m\u001b[1;33m==\u001b[0m\u001b[0mmax_iters\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 223\u001b[1;33m         \u001b[0mlosses\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mestimate_loss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    224\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    225\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\autograd\\grad_mode.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     25\u001b[0m         \u001b[1;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m             \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclone\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 27\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     28\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mF\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_14284\\730405776.py\u001b[0m in \u001b[0;36mestimate_loss\u001b[1;34m()\u001b[0m\n\u001b[0;32m     60\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mk\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0meval_iters\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     61\u001b[0m             \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_batch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 62\u001b[1;33m             \u001b[0mlogits\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     63\u001b[0m             \u001b[0mlosses\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     64\u001b[0m         \u001b[0mout\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlosses\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1192\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1193\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1194\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1195\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1196\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_14284\\730405776.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, idx, targets)\u001b[0m\n\u001b[0;32m    173\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    174\u001b[0m         \u001b[1;31m# Block component\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 175\u001b[1;33m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mblocks\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# (B, T, head_size)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    176\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    177\u001b[0m         \u001b[1;31m# final layer norm\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1192\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1193\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1194\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1195\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1196\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\container.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    202\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    203\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 204\u001b[1;33m             \u001b[0minput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    205\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    206\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1192\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1193\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1194\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1195\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1196\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_14284\\730405776.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    141\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    142\u001b[0m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mx\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msa_heads\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mln1\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# in original paper the layernorm is applied after the computation, though overtime it has become more common to apply it before the computaion\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 143\u001b[1;33m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mx\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mffwd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mln2\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    144\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    145\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1192\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1193\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1194\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1195\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1196\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_14284\\730405776.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    123\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    124\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 125\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnet\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    126\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    127\u001b[0m \u001b[1;32mclass\u001b[0m \u001b[0mBlock\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mModule\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1192\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1193\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1194\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1195\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1196\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\container.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    202\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    203\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 204\u001b[1;33m             \u001b[0minput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    205\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    206\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1192\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1193\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1194\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1195\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1196\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\activation.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    100\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    101\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 102\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minplace\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minplace\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    103\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    104\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\functional.py\u001b[0m in \u001b[0;36mrelu\u001b[1;34m(input, inplace)\u001b[0m\n\u001b[0;32m   1455\u001b[0m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrelu_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1456\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1457\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1458\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1459\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "# hyperparameters\n",
    "batch_size = 64 # how many independent sequences will we process in parallel?\n",
    "block_size = 256 # what is the maximum context length for predictions?\n",
    "max_iters = 5000\n",
    "learning_rate = 3e-4 #self-attention can't tolerate high lr\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu' # if you have gpu, run on it\n",
    "eval_interval = 500\n",
    "eval_iters = 200\n",
    "n_embd = 384\n",
    "n_head = 6\n",
    "n_layer = 6\n",
    "dropout = 0.2\n",
    "# ------------\n",
    "\n",
    "torch.manual_seed(1337)\n",
    "\n",
    "# wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
    "with open('input.txt', 'r', encoding='utf-8') as f:\n",
    "    text = f.read()\n",
    "\n",
    "# here are all the unique characters that occur in this text\n",
    "chars = sorted(list(set(text)))\n",
    "vocab_size = len(chars)\n",
    "# create a mapping from characters to integers\n",
    "stoi = { ch:i for i,ch in enumerate(chars) }\n",
    "itos = { i:ch for i,ch in enumerate(chars) }\n",
    "encode = lambda s: [stoi[c] for c in s] # encoder: take a string, output a list of integers\n",
    "decode = lambda l: ''.join([itos[i] for i in l]) # decoder: take a list of integers, output a string\n",
    "\n",
    "# Train and test splits\n",
    "data = torch.tensor(encode(text), dtype=torch.long)\n",
    "n = int(0.9*len(data)) # first 90% will be train, rest val\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]\n",
    "\n",
    "# data loading\n",
    "def get_batch(split):\n",
    "    # generate a small batch of data of inputs x and targets y\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
    "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
    "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
    "    x, y = x.to(device), y.to(device) # sending data to device (cpu or gpu)\n",
    "    return x, y\n",
    "\n",
    "@torch.no_grad()\n",
    "def estimate_loss():\n",
    "    out = {}\n",
    "    '''\n",
    "        Instead of evaluating the model every iter, \n",
    "        we Evaluate every eval_interval based on what the model has been trained so far\n",
    "    '''\n",
    "    model.eval() # Switch to Evaluation mode\n",
    "    for split in ['train', 'val']:\n",
    "        losses = torch.zeros(eval_iters)\n",
    "        for k in range(eval_iters):\n",
    "            X, Y = get_batch(split)\n",
    "            logits, loss = model(X, Y)\n",
    "            losses[k] = loss.item()\n",
    "        out[split] = losses.mean()\n",
    "    model.train() # Switch to Training mode\n",
    "    return out\n",
    "\n",
    "# Self-Attention\n",
    "class Head(nn.Module):\n",
    "    \"\"\" one head of self-attention \"\"\"\n",
    "    \n",
    "    def __init__(self, head_size):\n",
    "        super().__init__()\n",
    "        self.head_size = head_size\n",
    "        self.key = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.query = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.value = nn.Linear(n_embd, head_size, bias=False)        \n",
    "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size))) #tril is not a parameter to be optimized\n",
    "    \n",
    "        self.dropout = nn.Dropout(dropout)    \n",
    "        \n",
    "    def forward(self, x):\n",
    "        B, T, C = x.shape\n",
    "        k = self.key(x)   # (B, T, head_size)\n",
    "        q = self.query(x) # (B, T, head_size)\n",
    "        v = self.value(x) # (B, T, head_size)\n",
    "        \n",
    "        # compute attention scores (\"affinities\")\n",
    "        wei = (q @ k.transpose(-2, -1)) * self.head_size**-0.5   # (B, T, head_size) @ (B, head_size, T) -> (B, T, T)\n",
    "        wei = wei.masked_fill(self.tril[:T, :T]==0, float('-inf'))   # (B, T, T)\n",
    "        wei = F.softmax(wei, dim=-1)  # (B, T, T)\n",
    "        wei = self.dropout(wei)\n",
    "        \n",
    "        # perform weighted aggregation of values\n",
    "        out = wei @ v  # (B, T, T) @ (B, T, head_size) -> (B, T, head_size) == (B, T, n_embd)\n",
    "        return out\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\" multiple heads of self-attention in parallel \"\"\"\n",
    "    \n",
    "    def __init__ (self, num_heads, head_size):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
    "        self.proj = nn.Linear(n_embd, n_embd)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = torch.cat([h(x) for h in self.heads], dim=-1) # (B, T, head_size*4) == (B, T, n_embd)\n",
    "        out = self.proj(out) # we are doing Linear transformation of the output from self-attention\n",
    "        out = self.dropout(out)\n",
    "        return out\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    \"\"\" Simple Linear Layer followed by Non-Linearity\"\"\"\n",
    "    def __init__(self, n_embd):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(n_embd, 4* n_embd), # making ouput of Linear 4 times, as suggested in paper\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4 * n_embd, n_embd), # self.proj # here again making ouput of Linear n_embd, as suggested in paper\n",
    "            nn.Dropout(dropout)   \n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.net(x)    \n",
    "\n",
    "class Block(nn.Module):\n",
    "    \"\"\" Transformer block: communication followed by computation \"\"\"\n",
    "    def __init__(self, n_embd, n_head):\n",
    "        super().__init__()\n",
    "        # n_embd: embedding dimension, n_head: the number of heads we'd like\n",
    "        # Self Attention\n",
    "        head_size = n_embd // n_head\n",
    "        self.sa_heads = MultiHeadAttention(n_head, head_size)\n",
    "        # Feed Forward\n",
    "        self.ffwd = FeedForward(n_embd)\n",
    "        # Layer Norms\n",
    "        self.ln1 = nn.LayerNorm(n_embd) # making Unit Gaussians\n",
    "        self.ln2 = nn.LayerNorm(n_embd)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = x + self.sa_heads(self.ln1(x)) # in original paper the layernorm is applied after the computation, though overtime it has become more common to apply it before the computaion\n",
    "        x = x + self.ffwd(self.ln2(x))\n",
    "        return x\n",
    "        \n",
    "# super simple bigram model\n",
    "class BigramLanguageModel(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # Each word or token will be represented by an n_embd-dimensional embedding vector.\n",
    "        self.token_embedding_table = nn.Embedding(num_embeddings=vocab_size, embedding_dim=n_embd)\n",
    "        # We don't just want to encode the identity of token, but also its position\n",
    "        self.position_embedding_table = nn.Embedding(num_embeddings=block_size, embedding_dim=n_embd)\n",
    "        # Block Component\n",
    "        self.blocks = nn.Sequential(*[Block(n_embd, n_head=n_head) for _ in range(n_layer)])\n",
    "        # Layer Norm\n",
    "        self.ln_f = nn.LayerNorm(n_embd) # final layer norm\n",
    "        # Applies a linear transformation to the incoming data: :math:`y = x @ W.T + b`\n",
    "        self.lm_head = nn.Linear(in_features=n_embd, out_features=vocab_size)\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        B, T = idx.shape\n",
    "        \n",
    "        # idx and targets are both (B,T) tensor of integers\n",
    "        tkn_emb = self.token_embedding_table(idx) # (B,T,C)\n",
    "        \n",
    "        # range(0, T):: Every position will have n_embd-dimensional embedding vector.\n",
    "        pos_emb = self.position_embedding_table(torch.arange(T, device=device)) # (T,C)\n",
    "        \n",
    "        # x now, not just hold token identity but also its position\n",
    "        x = tkn_emb + pos_emb # (B, T, C)\n",
    "        \n",
    "        # Block component\n",
    "        x = self.blocks(x) # (B, T, head_size)\n",
    "        \n",
    "        # final layer norm\n",
    "        x = self.ln_f(x)\n",
    "        \n",
    "        # Making Logits\n",
    "        logits = self.lm_head(x) # (B,T,vocab_size)\n",
    "        \n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C)\n",
    "            targets = targets.view(B*T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "\n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        # idx is (B, T) array of indices in the current context\n",
    "        for _ in range(max_new_tokens):\n",
    "            # as we are implementing position embedding (pos_emb), we can't include context more than block_size\n",
    "            # crop idx to the last block_size token\n",
    "            idx_cond = idx[:, -block_size:] # (B, block_size) == (B, T)\n",
    "            # get the predictions\n",
    "            logits, loss = self(idx_cond) #forward()\n",
    "            # focus only on the last time step\n",
    "            logits = logits[:, -1, :] # becomes (B, C)\n",
    "            # apply softmax to get probabilities\n",
    "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
    "            # sample from the distribution\n",
    "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
    "            # append sampled index to the running sequence\n",
    "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
    "        return idx\n",
    "\n",
    "# Build Model\n",
    "model = BigramLanguageModel()\n",
    "m = model.to(device)\n",
    "\n",
    "# create a PyTorch optimizer\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Epochs\n",
    "for iter in range(max_iters):\n",
    "    \n",
    "    # After every eval_interval iters, evaluate the loss on train and val sets\n",
    "    if (iter % eval_interval == 0) or (iter==max_iters-1):\n",
    "        losses = estimate_loss()\n",
    "        print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
    "\n",
    "    # sample a batch of data\n",
    "    xb, yb = get_batch('train')\n",
    "\n",
    "    # train model\n",
    "    logits, loss = model.forward(xb, yb)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "# generate from the model\n",
    "context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
    "print(decode(m.generate(context, max_new_tokens=500)[0].tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5672ed93",
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Model Output when ran on Google Colab with GPU (took ~ 45min)\n",
    "    step 0: train loss 4.2846, val loss 4.2820\n",
    "    step 500: train loss 1.8865, val loss 2.0023\n",
    "    step 1000: train loss 1.5361, val loss 1.7221\n",
    "    step 1500: train loss 1.3948, val loss 1.6038\n",
    "    step 2000: train loss 1.3077, val loss 1.5490\n",
    "    step 2500: train loss 1.2523, val loss 1.5153\n",
    "    step 3000: train loss 1.2010, val loss 1.4894\n",
    "    step 3500: train loss 1.1587, val loss 1.4800\n",
    "    step 4000: train loss 1.1222, val loss 1.4800\n",
    "    step 4500: train loss 1.0853, val loss 1.4736\n",
    "    step 4999: train loss 1.0494, val loss 1.4913\n",
    "\n",
    "    But with price of a breast sast-creature.\n",
    "    Of whom, Cariolanus: of God! what's Romeo?\n",
    "\n",
    "    Third Consciden:\n",
    "    Mistress, let's proceed. Go to me, go: I say.\n",
    "\n",
    "    CAMILLO:\n",
    "    By my lord, I'll braw a light:\n",
    "    I have bore alone death.\n",
    "\n",
    "    SLY:\n",
    "    If you would I wish vengeance me when I was off\n",
    "    these advancementary; this to fled till\n",
    "    I clear thee join till ar outrain, thou art to me;\n",
    "    And, not many hath punishmen.\n",
    "\n",
    "    SLY:\n",
    "    They Gentleman, I have deliver'd by thus leave\n",
    "    He known I dissevel to you!\n",
    "\n",
    "    Lord:\n",
    "    Lords, am I though for\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4c77834",
   "metadata": {},
   "source": [
    "# Load the Pytorch Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "e953a2f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BigramLanguageModel(\n",
       "  (token_embedding_table): Embedding(65, 384)\n",
       "  (position_embedding_table): Embedding(256, 384)\n",
       "  (blocks): Sequential(\n",
       "    (0): Block(\n",
       "      (sa_heads): MultiHeadAttention(\n",
       "        (heads): ModuleList(\n",
       "          (0): Head(\n",
       "            (key): Linear(in_features=384, out_features=64, bias=False)\n",
       "            (query): Linear(in_features=384, out_features=64, bias=False)\n",
       "            (value): Linear(in_features=384, out_features=64, bias=False)\n",
       "            (dropout): Dropout(p=0.2, inplace=False)\n",
       "          )\n",
       "          (1): Head(\n",
       "            (key): Linear(in_features=384, out_features=64, bias=False)\n",
       "            (query): Linear(in_features=384, out_features=64, bias=False)\n",
       "            (value): Linear(in_features=384, out_features=64, bias=False)\n",
       "            (dropout): Dropout(p=0.2, inplace=False)\n",
       "          )\n",
       "          (2): Head(\n",
       "            (key): Linear(in_features=384, out_features=64, bias=False)\n",
       "            (query): Linear(in_features=384, out_features=64, bias=False)\n",
       "            (value): Linear(in_features=384, out_features=64, bias=False)\n",
       "            (dropout): Dropout(p=0.2, inplace=False)\n",
       "          )\n",
       "          (3): Head(\n",
       "            (key): Linear(in_features=384, out_features=64, bias=False)\n",
       "            (query): Linear(in_features=384, out_features=64, bias=False)\n",
       "            (value): Linear(in_features=384, out_features=64, bias=False)\n",
       "            (dropout): Dropout(p=0.2, inplace=False)\n",
       "          )\n",
       "          (4): Head(\n",
       "            (key): Linear(in_features=384, out_features=64, bias=False)\n",
       "            (query): Linear(in_features=384, out_features=64, bias=False)\n",
       "            (value): Linear(in_features=384, out_features=64, bias=False)\n",
       "            (dropout): Dropout(p=0.2, inplace=False)\n",
       "          )\n",
       "          (5): Head(\n",
       "            (key): Linear(in_features=384, out_features=64, bias=False)\n",
       "            (query): Linear(in_features=384, out_features=64, bias=False)\n",
       "            (value): Linear(in_features=384, out_features=64, bias=False)\n",
       "            (dropout): Dropout(p=0.2, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "        (dropout): Dropout(p=0.2, inplace=False)\n",
       "      )\n",
       "      (ffwd): FeedForward(\n",
       "        (net): Sequential(\n",
       "          (0): Linear(in_features=384, out_features=1536, bias=True)\n",
       "          (1): ReLU()\n",
       "          (2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "          (3): Dropout(p=0.2, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (ln1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "      (ln2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (1): Block(\n",
       "      (sa_heads): MultiHeadAttention(\n",
       "        (heads): ModuleList(\n",
       "          (0): Head(\n",
       "            (key): Linear(in_features=384, out_features=64, bias=False)\n",
       "            (query): Linear(in_features=384, out_features=64, bias=False)\n",
       "            (value): Linear(in_features=384, out_features=64, bias=False)\n",
       "            (dropout): Dropout(p=0.2, inplace=False)\n",
       "          )\n",
       "          (1): Head(\n",
       "            (key): Linear(in_features=384, out_features=64, bias=False)\n",
       "            (query): Linear(in_features=384, out_features=64, bias=False)\n",
       "            (value): Linear(in_features=384, out_features=64, bias=False)\n",
       "            (dropout): Dropout(p=0.2, inplace=False)\n",
       "          )\n",
       "          (2): Head(\n",
       "            (key): Linear(in_features=384, out_features=64, bias=False)\n",
       "            (query): Linear(in_features=384, out_features=64, bias=False)\n",
       "            (value): Linear(in_features=384, out_features=64, bias=False)\n",
       "            (dropout): Dropout(p=0.2, inplace=False)\n",
       "          )\n",
       "          (3): Head(\n",
       "            (key): Linear(in_features=384, out_features=64, bias=False)\n",
       "            (query): Linear(in_features=384, out_features=64, bias=False)\n",
       "            (value): Linear(in_features=384, out_features=64, bias=False)\n",
       "            (dropout): Dropout(p=0.2, inplace=False)\n",
       "          )\n",
       "          (4): Head(\n",
       "            (key): Linear(in_features=384, out_features=64, bias=False)\n",
       "            (query): Linear(in_features=384, out_features=64, bias=False)\n",
       "            (value): Linear(in_features=384, out_features=64, bias=False)\n",
       "            (dropout): Dropout(p=0.2, inplace=False)\n",
       "          )\n",
       "          (5): Head(\n",
       "            (key): Linear(in_features=384, out_features=64, bias=False)\n",
       "            (query): Linear(in_features=384, out_features=64, bias=False)\n",
       "            (value): Linear(in_features=384, out_features=64, bias=False)\n",
       "            (dropout): Dropout(p=0.2, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "        (dropout): Dropout(p=0.2, inplace=False)\n",
       "      )\n",
       "      (ffwd): FeedForward(\n",
       "        (net): Sequential(\n",
       "          (0): Linear(in_features=384, out_features=1536, bias=True)\n",
       "          (1): ReLU()\n",
       "          (2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "          (3): Dropout(p=0.2, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (ln1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "      (ln2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (2): Block(\n",
       "      (sa_heads): MultiHeadAttention(\n",
       "        (heads): ModuleList(\n",
       "          (0): Head(\n",
       "            (key): Linear(in_features=384, out_features=64, bias=False)\n",
       "            (query): Linear(in_features=384, out_features=64, bias=False)\n",
       "            (value): Linear(in_features=384, out_features=64, bias=False)\n",
       "            (dropout): Dropout(p=0.2, inplace=False)\n",
       "          )\n",
       "          (1): Head(\n",
       "            (key): Linear(in_features=384, out_features=64, bias=False)\n",
       "            (query): Linear(in_features=384, out_features=64, bias=False)\n",
       "            (value): Linear(in_features=384, out_features=64, bias=False)\n",
       "            (dropout): Dropout(p=0.2, inplace=False)\n",
       "          )\n",
       "          (2): Head(\n",
       "            (key): Linear(in_features=384, out_features=64, bias=False)\n",
       "            (query): Linear(in_features=384, out_features=64, bias=False)\n",
       "            (value): Linear(in_features=384, out_features=64, bias=False)\n",
       "            (dropout): Dropout(p=0.2, inplace=False)\n",
       "          )\n",
       "          (3): Head(\n",
       "            (key): Linear(in_features=384, out_features=64, bias=False)\n",
       "            (query): Linear(in_features=384, out_features=64, bias=False)\n",
       "            (value): Linear(in_features=384, out_features=64, bias=False)\n",
       "            (dropout): Dropout(p=0.2, inplace=False)\n",
       "          )\n",
       "          (4): Head(\n",
       "            (key): Linear(in_features=384, out_features=64, bias=False)\n",
       "            (query): Linear(in_features=384, out_features=64, bias=False)\n",
       "            (value): Linear(in_features=384, out_features=64, bias=False)\n",
       "            (dropout): Dropout(p=0.2, inplace=False)\n",
       "          )\n",
       "          (5): Head(\n",
       "            (key): Linear(in_features=384, out_features=64, bias=False)\n",
       "            (query): Linear(in_features=384, out_features=64, bias=False)\n",
       "            (value): Linear(in_features=384, out_features=64, bias=False)\n",
       "            (dropout): Dropout(p=0.2, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "        (dropout): Dropout(p=0.2, inplace=False)\n",
       "      )\n",
       "      (ffwd): FeedForward(\n",
       "        (net): Sequential(\n",
       "          (0): Linear(in_features=384, out_features=1536, bias=True)\n",
       "          (1): ReLU()\n",
       "          (2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "          (3): Dropout(p=0.2, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (ln1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "      (ln2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (3): Block(\n",
       "      (sa_heads): MultiHeadAttention(\n",
       "        (heads): ModuleList(\n",
       "          (0): Head(\n",
       "            (key): Linear(in_features=384, out_features=64, bias=False)\n",
       "            (query): Linear(in_features=384, out_features=64, bias=False)\n",
       "            (value): Linear(in_features=384, out_features=64, bias=False)\n",
       "            (dropout): Dropout(p=0.2, inplace=False)\n",
       "          )\n",
       "          (1): Head(\n",
       "            (key): Linear(in_features=384, out_features=64, bias=False)\n",
       "            (query): Linear(in_features=384, out_features=64, bias=False)\n",
       "            (value): Linear(in_features=384, out_features=64, bias=False)\n",
       "            (dropout): Dropout(p=0.2, inplace=False)\n",
       "          )\n",
       "          (2): Head(\n",
       "            (key): Linear(in_features=384, out_features=64, bias=False)\n",
       "            (query): Linear(in_features=384, out_features=64, bias=False)\n",
       "            (value): Linear(in_features=384, out_features=64, bias=False)\n",
       "            (dropout): Dropout(p=0.2, inplace=False)\n",
       "          )\n",
       "          (3): Head(\n",
       "            (key): Linear(in_features=384, out_features=64, bias=False)\n",
       "            (query): Linear(in_features=384, out_features=64, bias=False)\n",
       "            (value): Linear(in_features=384, out_features=64, bias=False)\n",
       "            (dropout): Dropout(p=0.2, inplace=False)\n",
       "          )\n",
       "          (4): Head(\n",
       "            (key): Linear(in_features=384, out_features=64, bias=False)\n",
       "            (query): Linear(in_features=384, out_features=64, bias=False)\n",
       "            (value): Linear(in_features=384, out_features=64, bias=False)\n",
       "            (dropout): Dropout(p=0.2, inplace=False)\n",
       "          )\n",
       "          (5): Head(\n",
       "            (key): Linear(in_features=384, out_features=64, bias=False)\n",
       "            (query): Linear(in_features=384, out_features=64, bias=False)\n",
       "            (value): Linear(in_features=384, out_features=64, bias=False)\n",
       "            (dropout): Dropout(p=0.2, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "        (dropout): Dropout(p=0.2, inplace=False)\n",
       "      )\n",
       "      (ffwd): FeedForward(\n",
       "        (net): Sequential(\n",
       "          (0): Linear(in_features=384, out_features=1536, bias=True)\n",
       "          (1): ReLU()\n",
       "          (2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "          (3): Dropout(p=0.2, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (ln1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "      (ln2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (4): Block(\n",
       "      (sa_heads): MultiHeadAttention(\n",
       "        (heads): ModuleList(\n",
       "          (0): Head(\n",
       "            (key): Linear(in_features=384, out_features=64, bias=False)\n",
       "            (query): Linear(in_features=384, out_features=64, bias=False)\n",
       "            (value): Linear(in_features=384, out_features=64, bias=False)\n",
       "            (dropout): Dropout(p=0.2, inplace=False)\n",
       "          )\n",
       "          (1): Head(\n",
       "            (key): Linear(in_features=384, out_features=64, bias=False)\n",
       "            (query): Linear(in_features=384, out_features=64, bias=False)\n",
       "            (value): Linear(in_features=384, out_features=64, bias=False)\n",
       "            (dropout): Dropout(p=0.2, inplace=False)\n",
       "          )\n",
       "          (2): Head(\n",
       "            (key): Linear(in_features=384, out_features=64, bias=False)\n",
       "            (query): Linear(in_features=384, out_features=64, bias=False)\n",
       "            (value): Linear(in_features=384, out_features=64, bias=False)\n",
       "            (dropout): Dropout(p=0.2, inplace=False)\n",
       "          )\n",
       "          (3): Head(\n",
       "            (key): Linear(in_features=384, out_features=64, bias=False)\n",
       "            (query): Linear(in_features=384, out_features=64, bias=False)\n",
       "            (value): Linear(in_features=384, out_features=64, bias=False)\n",
       "            (dropout): Dropout(p=0.2, inplace=False)\n",
       "          )\n",
       "          (4): Head(\n",
       "            (key): Linear(in_features=384, out_features=64, bias=False)\n",
       "            (query): Linear(in_features=384, out_features=64, bias=False)\n",
       "            (value): Linear(in_features=384, out_features=64, bias=False)\n",
       "            (dropout): Dropout(p=0.2, inplace=False)\n",
       "          )\n",
       "          (5): Head(\n",
       "            (key): Linear(in_features=384, out_features=64, bias=False)\n",
       "            (query): Linear(in_features=384, out_features=64, bias=False)\n",
       "            (value): Linear(in_features=384, out_features=64, bias=False)\n",
       "            (dropout): Dropout(p=0.2, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "        (dropout): Dropout(p=0.2, inplace=False)\n",
       "      )\n",
       "      (ffwd): FeedForward(\n",
       "        (net): Sequential(\n",
       "          (0): Linear(in_features=384, out_features=1536, bias=True)\n",
       "          (1): ReLU()\n",
       "          (2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "          (3): Dropout(p=0.2, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (ln1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "      (ln2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (5): Block(\n",
       "      (sa_heads): MultiHeadAttention(\n",
       "        (heads): ModuleList(\n",
       "          (0): Head(\n",
       "            (key): Linear(in_features=384, out_features=64, bias=False)\n",
       "            (query): Linear(in_features=384, out_features=64, bias=False)\n",
       "            (value): Linear(in_features=384, out_features=64, bias=False)\n",
       "            (dropout): Dropout(p=0.2, inplace=False)\n",
       "          )\n",
       "          (1): Head(\n",
       "            (key): Linear(in_features=384, out_features=64, bias=False)\n",
       "            (query): Linear(in_features=384, out_features=64, bias=False)\n",
       "            (value): Linear(in_features=384, out_features=64, bias=False)\n",
       "            (dropout): Dropout(p=0.2, inplace=False)\n",
       "          )\n",
       "          (2): Head(\n",
       "            (key): Linear(in_features=384, out_features=64, bias=False)\n",
       "            (query): Linear(in_features=384, out_features=64, bias=False)\n",
       "            (value): Linear(in_features=384, out_features=64, bias=False)\n",
       "            (dropout): Dropout(p=0.2, inplace=False)\n",
       "          )\n",
       "          (3): Head(\n",
       "            (key): Linear(in_features=384, out_features=64, bias=False)\n",
       "            (query): Linear(in_features=384, out_features=64, bias=False)\n",
       "            (value): Linear(in_features=384, out_features=64, bias=False)\n",
       "            (dropout): Dropout(p=0.2, inplace=False)\n",
       "          )\n",
       "          (4): Head(\n",
       "            (key): Linear(in_features=384, out_features=64, bias=False)\n",
       "            (query): Linear(in_features=384, out_features=64, bias=False)\n",
       "            (value): Linear(in_features=384, out_features=64, bias=False)\n",
       "            (dropout): Dropout(p=0.2, inplace=False)\n",
       "          )\n",
       "          (5): Head(\n",
       "            (key): Linear(in_features=384, out_features=64, bias=False)\n",
       "            (query): Linear(in_features=384, out_features=64, bias=False)\n",
       "            (value): Linear(in_features=384, out_features=64, bias=False)\n",
       "            (dropout): Dropout(p=0.2, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "        (dropout): Dropout(p=0.2, inplace=False)\n",
       "      )\n",
       "      (ffwd): FeedForward(\n",
       "        (net): Sequential(\n",
       "          (0): Linear(in_features=384, out_features=1536, bias=True)\n",
       "          (1): ReLU()\n",
       "          (2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "          (3): Dropout(p=0.2, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (ln1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "      (ln2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "  )\n",
       "  (ln_f): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "  (lm_head): Linear(in_features=384, out_features=65, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PATH = 'BigramModel.pth'\n",
    "model = torch.load(PATH, map_location=torch.device('cpu'))\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "0dd6e560",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Number of Parameters: 10788929\n"
     ]
    }
   ],
   "source": [
    "print(\"Total Number of Parameters:\", sum(p.numel() for p in m.parameters()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "e4026ecb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Lords:\n",
      "Have I trust'd through them not; for they do thee, shall\n",
      "She lie of those wildo: the of your lady foot.\n",
      "\n",
      "JULIET:\n",
      "I would excuse lords; proof, shall be hoft by arm:\n",
      "'Zoundsire I am his power and lean\n",
      "Uncalamal.' O God, and blow hath burnt I came!\n",
      "\n",
      "LADY ANNA:\n",
      "Here, Thou is, the pity of thee! a black not mock\n",
      "That noble daggers, not was mercy uncles;\n",
      "But guicide where I pray to his present,\n",
      "Wore hearted of and downfastest of his fells;\n",
      "The house servens o' the father field,\n",
      "See Keep, their E\n"
     ]
    }
   ],
   "source": [
    "# generate from the model\n",
    "context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
    "print(decode(model.generate(context, max_new_tokens=500)[0].tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "983e3dd2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
