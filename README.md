# Text-Word-Predictor-Bigram-Model

### Introduction:
In the realm of natural language processing (NLP), predictive text generation is a fundamental task with numerous applications, such as autocomplete, chatbots, and machine translation. The "Attention Is All You Need" paper introduced the revolutionary Transformer architecture, which has since become a cornerstone in modern NLP. This project aims to leverage the Transformer's attention mechanisms to build a Bigram language model that predicts the next word in a sentence given the previous words.

### ğŸ” Project Overview:
In the fast-paced world of predictive text generation, I've embarked on a journey to harness the power of the Transformer architecture, famously introduced in the "Attention Is All You Need" paper. <a href="https://arxiv.org/pdf/1706.03762.pdf">ğŸ“„</a> My goal? To create a cutting-edge bigram language model that predicts the next word in a sentence based on the two preceding words.


### ğŸ“Š Project Steps:
â€¢ Data Collection and Preprocessing: Gathered and cleaned diverse text data to fuel the model's training. <br/>
â€¢ Implementing the Transformer Architecture: Brought the core of the Transformer to life, featuring self-attention and feedforward neural networks. <br/>
â€¢ Creating the Bigram Language Model: Tailored the Transformer to handle bigram predictions, accommodating two preceding words. <br/>
â€¢ Training the Model: Trained the model, optimized with Adam, and kept a close eye on validation performance. <br/>
â€¢ Evaluation: Measured the model's success using metrics like perplexity and accuracy, comparing it against traditional n-grams. <br/>
â€¢ Fine-tuning and Optimization: Explored the hyperparameter landscape to unlock the model's full potential. <br/>
â€¢ Inference and Prediction: Transformed the model into a real-time next-word predictor, leveraging its learned contextual insights. <br/>

### ğŸŒŸ Impact and Future:
This project showcases the dynamic synergy between the Transformer architecture and Bigram-based language modeling. By expanding contextual understanding, we're stepping up the game in various NLP applications, from autocomplete to chatbots and beyond.

## ğŸŒŸ Sources:
ğŸ“ https://arxiv.org/pdf/1706.03762
<br>
ğŸï¸ https://www.youtube.com/watch?v=kCc8FmEb1nY

### Check out the Notebooks:
<div align="left">
  <a href="https://colab.research.google.com/github/lunaSnowflake/Text-Word-Predictor-Bigram-Model/blob/master/Bigram%20LLM%20Model%20-%20Final.ipynb">
    <img src="https://github.com/ultralytics/yolov5/releases/download/v1.0/logo-colab-small.png" width="10%" /></a>
    <img src="https://github.com/ultralytics/assets/raw/main/social/logo-transparent.png" width="5%" alt="" />
</div>

I am open to any suggestions, connect with me anywhere! <br/>
Also, I would appreciate it if I can get a ğŸŒŸ for this repository from your side. â˜º

## ğŸ’» Tech Stack:
![Python](https://img.shields.io/badge/python-3670A0?style=for-the-badge&logo=python&logoColor=ffdd54) 
![PyTorch](https://img.shields.io/badge/PyTorch-%23EE4C2C.svg?style=for-the-badge&logo=PyTorch&logoColor=white)

## ğŸ’« About Me:
![Dev Gif](https://media.giphy.com/media/f3iwJFOVOwuy7K6FFw/giphy.gif) <br/>

I am a Data Scientist/Analyst and a Developer <br/>
Check out my [GitHub](https://github.com/lunaSnowflake) profile for more details! See you on the other side :)

## ğŸŒ Socials:
[![LinkedIn](https://img.shields.io/badge/LinkedIn-%230077B5.svg?logo=linkedin&logoColor=white)](https://www.linkedin.com/in/hussainkhatumdi/) 
[![Kaggle](https://img.shields.io/badge/Kaggle-035a7d?logo=kaggle&logoColor=white)](https://www.kaggle.com/lunaticsain)
[![Medium](https://img.shields.io/badge/Medium-12100E?logo=medium&logoColor=white)](https://medium.com/@hussainkhatumadi53) 
[![Twitter](https://img.shields.io/badge/Twitter-%231DA1F2.svg?logo=Twitter&logoColor=white)](https://twitter.com/lunatic_sain) 
